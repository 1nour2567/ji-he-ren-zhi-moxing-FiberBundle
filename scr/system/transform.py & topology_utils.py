"""
æ‹“æ‰‘è¯Šæ–­æ¨¡å—ï¼šå®ç°é™ˆç±»æ¯”å€¼åˆ†ç±»å™¨ (Chern Ratio Classifier)
èŒè´£: é¢„è®­ç»ƒä¸€ä¸ª SVM åˆ†ç±»å™¨ï¼Œç”¨äºå®æ—¶è¯Šæ–­ L1 æ¨¡å‹çš„æ‹“æ‰‘çŠ¶æ€ã€‚
"""
# diagnoser.py
import numpy as np
import torch
import torch.nn as nn
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from typing import List, Tuple, Dict, Any

# å‡è®¾æ‚¨å·²å°† ChernClassCalculator æ”¾åœ¨ topology_utils.py ä¸­å¹¶å¯¼å…¥
# from .topology_utils import ChernClassCalculator
# ç®€åŒ–å¤„ç†ï¼šæ­¤å¤„æˆ‘ä»¬ç›´æ¥æ¨¡æ‹Ÿä¸€ä¸ªèƒ½äº§ç”Ÿç‰¹å¾çš„æ¨¡å‹ç»“æ„ for data generation
class SimplifiedTopologyModel(nn.Module):
    """
    ç®€åŒ–æ‹“æ‰‘æ¨¡å‹ï¼šç”¨äºæ¨¡æ‹Ÿç”Ÿæˆè®­ç»ƒåˆ†ç±»å™¨çš„ç³»ç»ŸçŠ¶æ€ã€‚
    å®ƒå¿…é¡»èƒ½å¤Ÿè¿è¡Œå¹¶äº§ç”Ÿ c1, c2, c2/c1 çš„ç»Ÿè®¡é‡ã€‚
    ï¼ˆåœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é€šå¸¸æ˜¯æ‚¨çš„ CognitiveFiberBundle æ¨¡å‹çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
    """
    def __init__(self, d_model: int, inputs_tensor=None):
        super().__init__()
        self.d_model = d_model
        self.inputs_tensor = inputs_tensor  # å­˜å‚¨ç‰¹å®šè¾“å…¥å¼ é‡

    def forward(self, x: torch.Tensor) -> Dict[str, float]:
        """
        è¿è¡Œå‰å‘ä¼ æ’­å¹¶è¿”å›æ¨¡æ‹Ÿæˆ–è®¡ç®—çš„æ‹“æ‰‘ç‰¹å¾ã€‚
        ç”±äºæ— æ³•åœ¨æ­¤å¤„è¿è¡Œ ChernClassCalculatorï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿç‰¹å¾ã€‚
        """
        # --- æ¨¡æ‹Ÿè®¡ç®—è¿‡ç¨‹ ---
        # B, N, D = x.shape
        # connection = self.chern_calculator.compute_connection_form(x)
        # curvature = self.chern_calculator.compute_curvature_form(connection)
        # chern_info = self.chern_calculator.compute_chern_classes(curvature)

        # ç®€åŒ–ï¼šä½¿ç”¨è¾“å…¥ x çš„ç»Ÿè®¡ç‰¹å¾æ¥æ¨¡æ‹Ÿæ‹“æ‰‘ä¸å˜é‡
        x_norm = x.norm().item()
        x_std = x.std().item()
        x_mean = x.mean().item()

        # æ ¹æ®è¾“å…¥ç‰¹å¾ï¼ˆx_norm, x_stdï¼‰æ˜ å°„åˆ° c1, c2, ratio çš„ç»Ÿè®¡é‡
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ˜ å°„ï¼Œå®é™…ä¸­åº”ç”± ChernClassCalculator äº§ç”Ÿ
        c1_mean = abs(x_mean) * 0.5 + 0.001  # æ”¾å¤§ç³»æ•°
        c2_mean = (x_std * 0.8) + 0.001      # æ”¾å¤§ç³»æ•°
        ratio_mean = c2_mean / (c1_mean + 1e-8)
        c1_std = x_std / 10.0
        ratio_std = abs(x_norm) / 100.0

        return {
             'c1_mean': c1_mean,
             'c2_mean': c2_mean,
             'ratio_mean': ratio_mean,
             'c1_std': c1_std,
             'ratio_std': ratio_std,
        }

    def get_topo_features(self) -> Dict[str, float]:
        """
        è·å–é¢„è®¾è¾“å…¥çš„æ‹“æ‰‘ç‰¹å¾ï¼Œç”¨äºè®­ç»ƒåˆ†ç±»å™¨
        """
        if self.inputs_tensor is not None:
            return self.forward(self.inputs_tensor)
        else:
            # å¦‚æœæ²¡æœ‰é¢„è®¾è¾“å…¥ï¼Œä½¿ç”¨éšæœºè¾“å…¥
            random_input = torch.randn(1, 10, self.d_model)
            return self.forward(random_input)


class ChernRatioClassifier:
    """
    é™ˆç±»æ¯”å€¼åˆ†ç±»å™¨ (L2 è¯Šæ–­å¤§è„‘)ã€‚
    ä¼˜åŒ–: ä½¿ç”¨ 'poly' æ ¸å’Œæ›´é«˜çš„ C å€¼ã€‚
    """
    def __init__(self, classifier_type: str = 'svm'):
        # ä¼˜åŒ–åçš„ SVM é…ç½®
        if classifier_type == 'svm':
            self.classifier = SVC(
                kernel='poly',         # æ›´æ”¹ä¸ºå¤šé¡¹å¼æ ¸
                degree=3,              # å¤šé¡¹å¼æ¬¡æ•°
                C=10.0,                # å¢å¤§æƒ©ç½šç³»æ•° Cï¼Œå¢åŠ å¯¹è¯¯åˆ†ç±»çš„æ•æ„Ÿæ€§
                probability=True,
                random_state=42
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»å™¨ç±»å‹: {classifier_type}")

        self.scaler = None
        #self.feature_names = ['c1_mean', 'c2_mean', 'ratio_mean', 'c1_std', 'ratio_std']
        self.feature_names = ['c1_mean', 'c2_mean', 'ratio_mean']
        #pretrained_diagnoser = ChernRatioClassifier(classifier_type='svm') # ğŸ’¥ å®ä¾‹åŒ– L2 è¯Šæ–­å™¨

    def extract_chern_ratio_features(self, systems: List[SimplifiedTopologyModel]) -> np.ndarray:
        """
        ä»ä¸€ç»„æ¨¡å‹å®ä¾‹ä¸­æå–æ‹“æ‰‘ç‰¹å¾ï¼Œå¹¶å±•å¹³ä¸º [N_samples, N_features] çŸ©é˜µã€‚
        æ³¨æ„: åœ¨ Transformer çš„ä¸»è®­ç»ƒå¾ªç¯ä¸­ï¼Œè¾“å…¥å°†æ˜¯ model.collect_topo_features() çš„ç»“æœã€‚
        """
        all_features = []
        for model in systems:
            # è·å–é¢„è®¾çš„æ‹“æ‰‘ç‰¹å¾
            topo_dict = model.get_topo_features()

            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
            feature_vector = [topo_dict[name] for name in self.feature_names]
            multi_layer_features = feature_vector * NUM_LAYERS
            all_features.append(multi_layer_features)

        return np.array(all_features)

    def fit(self, systems: List[SimplifiedTopologyModel], labels: np.ndarray):
        """è®­ç»ƒåˆ†ç±»å™¨"""
        X = self.extract_chern_ratio_features(systems)
        y = labels

        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)

        self.classifier.fit(X_scaled, y)

    def predict(self, input_features: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """
        é¢„æµ‹æ¨¡å‹çŠ¶æ€ã€‚
        è¾“å…¥ input_features: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å±•å¹³åçš„ç‰¹å¾å‘é‡ (ä¾‹å¦‚æ¥è‡ª model.collect_topo_features())ã€‚
        """
        # input_features å·²ç»æ˜¯ np.ndarray æ•°ç»„ (æˆ–å¯è½¬æ¢ä¸ºæ•°ç»„)
        X_test = np.array(input_features).squeeze()

        # å¦‚æœè¾“å…¥æ˜¯å•ä¸ªæ ·æœ¬ (ä¸€ç»´å‘é‡)ï¼Œå°†å…¶ reshape ä¸º (1, -1)
        if X_test.ndim == 1:
            X_test = X_test.reshape(1, -1)

        if self.scaler is None:
            raise RuntimeError("åˆ†ç±»å™¨å°šæœªè®­ç»ƒ (Scaler æœªåˆå§‹åŒ–)ã€‚è¯·å…ˆè¿è¡Œ fitã€‚")

        # åº”ç”¨è®­ç»ƒæ—¶çš„æ ‡å‡†åŒ–
        X_scaled = self.scaler.transform(X_test)

        predictions = self.classifier.predict(X_scaled)
        probabilities = self.classifier.predict_proba(X_scaled)

        return predictions, probabilities


# =======================================================
# æ•°æ®ç”Ÿæˆä¸è®­ç»ƒå‡½æ•°
# =======================================================

def create_training_systems(vocab_size=8, d_model=16, n_samples_per_class=50) -> Tuple[List[SimplifiedTopologyModel], np.ndarray]:
    """
    åˆ›å»ºç”¨äºè®­ç»ƒ ChernRatioClassifier çš„æ¨¡æ‹Ÿè®¤çŸ¥ç³»ç»Ÿæ•°æ®ã€‚
    ä¼˜åŒ–: å¢åŠ çŠ¶æ€ 2 çš„æ ·æœ¬é‡ä»¥æé«˜æ•æ„Ÿæ€§ã€‚
    """

    n_samples_base = n_samples_per_class
    # è®¡ç®—æ€»æ ·æœ¬é‡: æ­£å¸¸(50) + å¼‚å¸¸(100) + çº¦æŸè¿å(100) = 250
    n_samples_state_0 = n_samples_base      # æ­£å¸¸ç³»ç»Ÿ: 50
    n_samples_state_1 = n_samples_base * 2  # å¼‚å¸¸ç³»ç»Ÿ: 100 (å¢åŠ )
    n_samples_state_2 = n_samples_base * 2  # çº¦æŸè¿åç³»ç»Ÿ: 100 (ä¿æŒé«˜ä½)
    total_samples = n_samples_state_0 + n_samples_state_1 + n_samples_state_2

    print(f"ğŸ§ª æ­£åœ¨åˆ›å»ºç”¨äºåˆ†ç±»å™¨è®­ç»ƒçš„ {total_samples} ä¸ªæ¨¡æ‹Ÿè®¤çŸ¥ç³»ç»Ÿ...")

    systems = []
    system_types = []
    batch_size = 1

    # ç±»åˆ« 0: æ­£å¸¸ç³»ç»Ÿ (n_samples_per_class)
    for _ in range(n_samples_per_class):
        inputs = torch.randn(batch_size, vocab_size, d_model) * np.random.uniform(0.5, 1.5)
        # ç›®æ ‡ï¼šè®© c1 mean åœ¨ 0.4 åˆ° 30.0 ä¹‹é—´ (é€šè¿‡è°ƒæ•´ inputs çš„å…¨å±€å‡å€¼å®ç°)
        # ä½¿ç”¨ np.random.uniform(0.4, 30.0) ç¡®ä¿é«˜æ›²ç‡çŠ¶æ€è¢«çº³å…¥â€œæ­£å¸¸â€æ ·æœ¬
        mean_val = np.random.uniform(0.4, 30.0)
        inputs = inputs + torch.ones_like(inputs) * mean_val
        model = SimplifiedTopologyModel(d_model=d_model, inputs_tensor=inputs)
        systems.append(model)
        system_types.append(0)

    # ç±»åˆ« 1: å¼‚å¸¸ç³»ç»Ÿ (n_samples_per_class)
    for _ in range(n_samples_per_class):
        inputs = torch.randn(batch_size, vocab_size, d_model) * 0.5
        # åˆ¶é€ ç¦»ç¾¤å€¼
        inputs[0, 0, 0] = np.random.uniform(50, 100)
        inputs[0, 1, 5] = np.random.uniform(-100, -50)
        model = SimplifiedTopologyModel(d_model=d_model, inputs_tensor=inputs)
        systems.append(model)
        system_types.append(1)

    # ç±»åˆ« 2: çº¦æŸè¿åç³»ç»Ÿ (n_samples_state_2) - æ ·æœ¬é‡ç¿»å€
    for _ in range(n_samples_state_2):
        inputs = torch.randn(batch_size, vocab_size, d_model) * 0.1
        # åˆ¶é€ å…¨å±€é«˜å€¼
        inputs = inputs + torch.ones_like(inputs) * np.random.uniform(100, 200) # å¢å¤§å‡å€¼
        model = SimplifiedTopologyModel(d_model=d_model, inputs_tensor=inputs)
        systems.append(model)
        system_types.append(2)

    return systems, np.array(system_types)

def setup_and_train_diagnoser(d_model: int = 16) -> ChernRatioClassifier:
    """è®¾ç½®å¹¶è®­ç»ƒ ChernRatioClassifier"""

    # 1. ç”Ÿæˆæ•°æ®
    all_systems, all_labels = create_training_systems(d_model=d_model)

    # 2. åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_systems, test_systems, train_labels, test_labels = train_test_split(
        all_systems, all_labels, test_size=0.2, random_state=42, stratify=all_labels
    )

    print(f"   è®­ç»ƒé›†å¤§å°: {len(train_systems)}, æµ‹è¯•é›†å¤§å°: {len(test_systems)}")

    # 3. åˆå§‹åŒ–å¹¶è®­ç»ƒåˆ†ç±»å™¨
    pretrained_diagnoser = ChernRatioClassifier(classifier_type='svm')

    print("   ğŸš€ å¼€å§‹è®­ç»ƒ SVM è¯Šæ–­å™¨...")
    pretrained_diagnoser.fit(train_systems, train_labels)

    # 4. è¯„ä¼° (æ¨è)
    # æ³¨æ„: test_systems çš„ forward éœ€è¦è¢«å†æ¬¡è°ƒç”¨ä»¥ç”Ÿæˆç‰¹å¾
    X_test_features = pretrained_diagnoser.extract_chern_ratio_features(test_systems)
    predictions, _ = pretrained_diagnoser.predict(X_test_features)
    accuracy = accuracy_score(test_labels, predictions)

    print(f"   ğŸ‰ è¯Šæ–­å™¨åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ (SVM): {accuracy*100:.2f}%")

    return pretrained_diagnoser

# =======================================================
# éªŒè¯ä»£ç  (åœ¨å®é™…éƒ¨ç½²ä¸­å¯æ³¨é‡Š)
# =======================================================
if __name__ == "__main__":

    # ... (è¿è¡Œ setup_and_train_diagnoser)
    pretrained_diagnoser = setup_and_train_diagnoser()

    # åŸºç¡€çš„ 5 ä¸ªç‰¹å¾æ¨¡æ¿
    #BASE_FEATURES_NORMAL = np.array([1.0, 0.5, 0.5, 0.1, 0.05])
    #BASE_FEATURES_ANOMALOUS = np.array([0.1, 5.0, 50.0, 0.5, 0.3])
    #BASE_FEATURES_CONSTRAINT = np.array([0.001, 0.1, 100.0, 0.01, 0.01])
    BASE_FEATURES_NORMAL = np.array([1.0, 0.5, 0.5])
    BASE_FEATURES_ANOMALOUS = np.array([0.1, 5.0, 50.0])
    BASE_FEATURES_CONSTRAINT = np.array([0.001, 0.1, 100.0])

    NUM_LAYERS = 6 # å¿…é¡»ä¸ diagnoser.py ä¸­çš„å®šä¹‰ä¸€è‡´

    # ğŸ’¥ ä¿®æ­£ç‚¹ï¼šå°† 5 ä¸ªç‰¹å¾é‡å¤ 6 æ¬¡ï¼Œä»¥åŒ¹é… L2 è¯Šæ–­å™¨çš„ 30 ç»´è¾“å…¥
    simulated_features_normal = np.tile(BASE_FEATURES_NORMAL, NUM_LAYERS)
    simulated_features_anomalous = np.tile(BASE_FEATURES_ANOMALOUS, NUM_LAYERS)
    simulated_features_constraint = np.tile(BASE_FEATURES_CONSTRAINT, NUM_LAYERS)

    states, _ = pretrained_diagnoser.predict([
        simulated_features_normal,
        simulated_features_anomalous,
        simulated_features_constraint
    ])

    print("\nå®æ—¶è¯Šæ–­æ¨¡æ‹Ÿ:")
    print(f"  æ­£å¸¸ç‰¹å¾è¯Šæ–­ç»“æœ: {states[0]} (æœŸæœ› 0)")
    print(f"  å¼‚å¸¸ç‰¹å¾è¯Šæ–­ç»“æœ: {states[1]} (æœŸæœ› 1 æˆ– 2)")
    print(f"  çº¦æŸç‰¹å¾è¯Šæ–­ç»“æœ: {states[2]} (æœŸæœ› 2)")

    # æ·»åŠ æ ‡ç­¾è¯´æ˜
    print("\næ ‡ç­¾è¯´æ˜:")
    print("  0: æ­£å¸¸ç³»ç»Ÿ - æ‹“æ‰‘ç»“æ„ç¨³å®šï¼Œé™ˆç±»å€¼é€‚ä¸­")
    print("  1: å¼‚å¸¸ç³»ç»Ÿ - é«˜å±€éƒ¨æ›²ç‡ï¼Œc2/c1 æ¯”å€¼å¼‚å¸¸")
    print("  2: çº¦æŸè¿åç³»ç»Ÿ - å‡ ä½•ç»“æ„åˆšæ€§ï¼Œæ‹“æ‰‘çº¦æŸè¢«ç ´å")


