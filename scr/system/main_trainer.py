# =======================================================
# main_trainer.py
# æ ¸å¿ƒé›†æˆä¸Žè®­ç»ƒæ¨¡å—ï¼šå®žçŽ°å…ƒå­¦ä¹ åé¦ˆå›žè·¯
# èŒè´£: é¢„è®­ç»ƒè¯Šæ–­å™¨ï¼Œåˆå§‹åŒ– L1/L2 æ¨¡åž‹ï¼Œå¹¶æ‰§è¡Œå¸¦æœ‰ä¿®æ­£é€»è¾‘çš„ä¸»è®­ç»ƒã€‚
# =======================================================

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time

# --- å¯¼å…¥æ‰€æœ‰æ¨¡å— (å‡è®¾å®ƒä»¬ä½äºŽåŒä¸€ç›®å½•ä¸‹æˆ–å·²æ­£ç¡®é…ç½®Pythonè·¯å¾„) ---
# æ³¨æ„: åœ¨å®žé™…è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿è¿™äº›å¯¼å…¥è·¯å¾„æ˜¯æ­£ç¡®çš„
 # (Module I)


# -------------------------------------------------------
# è¾…åŠ©å‡½æ•°: æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
# -------------------------------------------------------
def get_mock_data_loader(vocab_size: int, seq_len: int, batch_size: int, num_batches: int):
    """ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿåºåˆ—æ•°æ®çš„è¿­ä»£å™¨"""
    for _ in range(num_batches):
        # æ¨¡æ‹Ÿè¾“å…¥åºåˆ— (B, N)
        inputs = torch.randint(0, vocab_size, (batch_size, seq_len))
        # æ¨¡æ‹Ÿç›®æ ‡åºåˆ— (ä¸‹ä¸€ä¸ªè¯é¢„æµ‹)
        targets = torch.roll(inputs, shifts=-1, dims=1)
        yield inputs, targets

# -------------------------------------------------------
# ä¸»è®­ç»ƒå‡½æ•°: å¸¦æœ‰å…ƒå­¦ä¹ æŒ‡å¯¼çš„è®­ç»ƒ
# -------------------------------------------------------

def train_with_meta_guidance(
    model: TopologyAwareTransformer, 
    diagnoser: ChernRatioClassifier, 
    corrector: TopologicalCorrector, 
    # ðŸ’¥ ä¿®æ”¹: ä¸å†ä¼ å…¥ data_loaderï¼Œè€Œæ˜¯ä¼ å…¥åˆ›å»º data_loader æ‰€éœ€çš„å‚æ•°
    get_data_loader_func, # æ–°å‚æ•°: ä¼ å…¥åˆ›å»º data_loader çš„å‡½æ•° (å³ get_mock_data_loader)
    data_loader_params: dict, # æ–°å‚æ•°: ä¼ å…¥åˆ›å»º data_loader æ‰€éœ€çš„å‚æ•° 
    epochs: int, 
    meta_check_freq: int = 50
):
    """
    å¸¦æœ‰é™ˆç±»è¯Šæ–­çš„å…ƒå­¦ä¹ è®­ç»ƒå¾ªçŽ¯ã€‚
    
    :param model: L1 å­¦ä¹ æ¨¡åž‹
    :param diagnoser: L2 è¯Šæ–­å™¨
    :param corrector: L2 ä¿®æ­£å™¨
    :param data_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
    :param epochs: è®­ç»ƒè½®æ•°
    :param meta_check_freq: è¿è¡Œ L2 è¯Šæ–­çš„é¢‘çŽ‡ (æ¯éš”å¤šå°‘ä¸ª batch)
    """
    optimizer = corrector.optimizer 
    criterion = nn.CrossEntropyLoss()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    print("\n===========================================================")
    print(f"ðŸš€ å¼€å§‹å…ƒå­¦ä¹ è®­ç»ƒ (Device: {device})")
    print(f"   L2 è¯Šæ–­é¢‘çŽ‡: æ¯ {meta_check_freq} æ­¥æ£€æŸ¥ä¸€æ¬¡")
    print("===========================================================")

    for epoch in range(1, epochs + 1):
        total_loss_epoch = 0
        total_task_loss = 0

        # ðŸ’¥ ä¿®æ­£ç‚¹ï¼šåœ¨æ¯ä¸ª Epoch å¼€å§‹æ—¶é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = get_data_loader_func(**data_loader_params)
        
        for batch_idx, (inputs, targets) in enumerate(data_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            
            # --- L1: ä¸»å­¦ä¹ ä»»åŠ¡ä¸Žæ‹“æ‰‘æ­£åˆ™åŒ– ---
            optimizer.zero_grad()
            outputs = model(inputs) # å‰å‘ä¼ æ’­è§¦å‘æ‹“æ‰‘ç‰¹å¾è®¡ç®—
            
            # 1. ä»»åŠ¡æŸå¤± (å¦‚è¯­è¨€å»ºæ¨¡)
            loss_task = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))
            
            # 2. æ‹“æ‰‘æ­£åˆ™åŒ–æŸå¤± (åŸºäºŽ c1 å‡å€¼)
            topo_loss_c1 = 0.0
            for layer in model.layers:
                # æ‹“æ‰‘æŸå¤±: æœ€å°åŒ– c1 çš„ç»å¯¹å€¼å‡å€¼
                topo_loss_c1 += torch.tensor(layer.topology_info['first_chern_class_mean']).abs()
            
            # 3. æ€»æŸå¤± = ä»»åŠ¡æŸå¤± + Î» * æ‹“æ‰‘æŸå¤±
            total_loss = loss_task + corrector.current_lambda * topo_loss_c1
            
            total_loss.backward()
            optimizer.step()
            
            total_loss_epoch += total_loss.item()
            total_task_loss += loss_task.item()
            
            # --- L2: å…ƒå­¦ä¹ è¯Šæ–­ä¸Žä¿®æ­£ ---
            if (batch_idx + 1) % meta_check_freq == 0:
                
                print(f"\n[Meta-Check] Epoch {epoch}/{epochs}, Step {batch_idx + 1}")
                print(f"  å½“å‰ L1 ä»»åŠ¡æŸå¤±: {loss_task.item():.4f}")
                print(f"  å½“å‰ Î»: {corrector.current_lambda:.6f}")
                
                # 1. æå– L1 ç‰¹å¾
                # ç‰¹å¾æ˜¯ NumPy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (1, N_features)
                topo_features = model.collect_topo_features()
                SCALE_CORRECTION_FACTOR = 1.0 / 21.5
                for i in range(topo_features.shape[1]):
                  # åªä¿®æ­£å‡å€¼å’Œæ ‡å‡†å·®ç‰¹å¾ (ç´¢å¼• 0, 1, 3, 4)
                  if i % 5 != 2:
                     topo_features[0, i] *= SCALE_CORRECTION_FACTOR# å¿½ç•¥ c2/c1 æ¯”å€¼ (ç´¢å¼• 2)

        
                
                # 2. L2 è¯Šæ–­
                predicted_states, _ = diagnoser.predict(topo_features)
                predicted_state = predicted_states[0]
                
                print(f"  ðŸ§  è¯Šæ–­å™¨é¢„æµ‹çŠ¶æ€: {predicted_state} (0:æ­£å¸¸, 1:å¼‚å¸¸, 2:çº¦æŸ)")

                # 3. æå– L1 æ‹“æ‰‘ä¿¡æ¯ (åŒ…å« c1 mean, c2/c1 mean ç­‰)
                # å‡è®¾æ‚¨çš„ transformer æ¨¡å—æœ‰ä¸€ä¸ª get_topo_info æ–¹æ³•æ¥è¿”å›ž self.topology_info
                current_topo_info = model.get_current_topo_info()
                corrector.execute_correction(predicted_state, current_topo_info)

                
            
            # ç®€å•çš„è¿›åº¦æ‰“å°
            if (batch_idx + 1) % 10 == 0:
                print(f"  [Epoch {epoch}] Step {batch_idx + 1} | Loss: {total_loss.item():.4f}", end='\r')

        avg_loss = total_loss_epoch / (batch_idx + 1)
        avg_task_loss = total_task_loss / (batch_idx + 1)
        
        print(f"\n\n--- Epoch {epoch} æ€»ç»“ ---")
        print(f"  å¹³å‡æ€»æŸå¤±: {avg_loss:.4f}")
        print(f"  å¹³å‡ä»»åŠ¡æŸå¤±: {avg_task_loss:.4f}")
        print(f"  å¹³å‡ç¬¬ä¸€é™ˆç±» (æœ€åŽ): {model.layers[0].topology_info.get('first_chern_class_mean', 0):.6f}")


# -------------------------------------------------------
# ä¸»è¿è¡Œé€»è¾‘
# -------------------------------------------------------
if __name__ == "__main__":
    # --- é…ç½®å‚æ•° ---
    VOCAB_SIZE = 5000
    D_MODEL = 128
    NUM_LAYERS = 6
    BATCH_SIZE = 32
    SEQ_LEN = 50
    NUM_BATCHES = 200 # æ¨¡æ‹Ÿæ€»è®­ç»ƒæ­¥æ•°
    EPOCHS = 3
    INITIAL_LAMBDA = 0.005
    META_CHECK_FREQ = 20 # æ¯ 20 æ­¥è¿›è¡Œä¸€æ¬¡å…ƒå­¦ä¹ è¯Šæ–­

    # ----------------------------------------
    # I. é¢„è®­ç»ƒ L2 è¯Šæ–­å™¨ (ChernRatioClassifier)
    # ----------------------------------------
    start_time = time.time()
    # è¯Šæ–­å™¨åªéœ€è¦ä¸€ä¸ªè¾ƒå°çš„ d_model å³å¯è®­ç»ƒå…¶åˆ†ç±»é€»è¾‘
    pretrained_diagnoser = setup_and_train_diagnoser(d_model=32) 
    print(f"\n>>> è¯Šæ–­å™¨é¢„è®­ç»ƒè€—æ—¶: {time.time() - start_time:.2f} ç§’ <<<")
    
    # ----------------------------------------
    # II. åˆå§‹åŒ– L1 æ¨¡åž‹å’Œä¿®æ­£å™¨
    # ----------------------------------------
    model = TopologyAwareTransformer(NUM_LAYERS, VOCAB_SIZE, D_MODEL)
    
    # æ‰“å°æ¨¡åž‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nåˆå§‹åŒ– L1 æ¨¡åž‹: TopologyAwareTransformer")
    print(f"  æ¨¡åž‹å‚æ•°æ€»æ•°: {total_params:,}")
    
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    corrector = TopologicalCorrector(model, optimizer, initial_lambda=INITIAL_LAMBDA)
    
    # ----------------------------------------
    # III. å¯åŠ¨å…ƒå­¦ä¹ è®­ç»ƒ
    # ----------------------------------------
    
    # æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
    #data_loader = get_mock_data_loader(VOCAB_SIZE, SEQ_LEN, BATCH_SIZE, NUM_BATCHES)
    loader_params = {
        'vocab_size': VOCAB_SIZE,
        'seq_len': SEQ_LEN,
        'batch_size': BATCH_SIZE,
        'num_batches': NUM_BATCHES
    }

    # å¼€å§‹è®­ç»ƒ
    train_with_meta_guidance(
        model, 
        pretrained_diagnoser, 
        corrector, 
        get_mock_data_loader,
        loader_params, 
        epochs=EPOCHS, 
        meta_check_freq=META_CHECK_FREQ
    )

    print("\n===========================================================")
    print("âœ¨ æ‹“æ‰‘å…ƒå­¦ä¹ ç³»ç»Ÿè®­ç»ƒå®Œæˆã€‚")
    print("===========================================================")
