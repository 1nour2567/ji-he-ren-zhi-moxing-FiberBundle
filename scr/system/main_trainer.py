# =======================================================
# main_trainer.py
# æ ¸å¿ƒé›†æˆä¸Žè®­ç»ƒæ¨¡å—ï¼šå®žçŽ°å…ƒå­¦ä¹ åé¦ˆå›žè·¯
# èŒè´£: é¢„è®­ç»ƒè¯Šæ–­å™¨ï¼Œåˆå§‹åŒ– L1/L2 æ¨¡åž‹ï¼Œå¹¶æ‰§è¡Œå¸¦æœ‰ä¿®æ­£é€»è¾‘çš„ä¸»è®­ç»ƒã€‚
# =======================================================

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time

# --- å¯¼å…¥æ‰€æœ‰æ¨¡å— (å‡è®¾å®ƒä»¬ä½äºŽåŒä¸€ç›®å½•ä¸‹æˆ–å·²æ­£ç¡®é…ç½®Pythonè·¯å¾„) ---
# æ³¨æ„: åœ¨å®žé™…è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿è¿™äº›å¯¼å…¥è·¯å¾„æ˜¯æ­£ç¡®çš„
 # (Module I)


# -------------------------------------------------------
# è¾…åŠ©å‡½æ•°: æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
# -------------------------------------------------------
def get_mock_data_loader(vocab_size: int, seq_len: int, batch_size: int, num_batches: int):
    """ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿåºåˆ—æ•°æ®çš„è¿­ä»£å™¨"""
    for _ in range(num_batches):
        # æ¨¡æ‹Ÿè¾“å…¥åºåˆ— (B, N)
        inputs = torch.randint(0, vocab_size, (batch_size, seq_len))
        # æ¨¡æ‹Ÿç›®æ ‡åºåˆ— (ä¸‹ä¸€ä¸ªè¯é¢„æµ‹)
        targets = torch.roll(inputs, shifts=-1, dims=1)
        yield inputs, targets

# -------------------------------------------------------
# ä¸»è®­ç»ƒå‡½æ•°: å¸¦æœ‰å…ƒå­¦ä¹ æŒ‡å¯¼çš„è®­ç»ƒ
# -------------------------------------------------------

def train_with_meta_guidance(
    model: TopologyAwareTransformer,
    diagnoser: ChernRatioClassifier,
    corrector: TopologicalCorrector,
    # ðŸ’¥ ä¿®æ”¹: ä¸å†ä¼ å…¥ data_loaderï¼Œè€Œæ˜¯ä¼ å…¥åˆ›å»º data_loader æ‰€éœ€çš„å‚æ•°
    get_data_loader_func, # æ–°å‚æ•°: ä¼ å…¥åˆ›å»º data_loader çš„å‡½æ•° (å³ get_mock_data_loader)
    data_loader_params: dict, # æ–°å‚æ•°: ä¼ å…¥åˆ›å»º data_loader æ‰€éœ€çš„å‚æ•°
    epochs: int,
    meta_check_freq: int = 50
):
    """
    å¸¦æœ‰é™ˆç±»è¯Šæ–­çš„å…ƒå­¦ä¹ è®­ç»ƒå¾ªçŽ¯ã€‚

    :param model: L1 å­¦ä¹ æ¨¡åž‹
    :param diagnoser: L2 è¯Šæ–­å™¨
    :param corrector: L2 ä¿®æ­£å™¨
    :param data_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
    :param epochs: è®­ç»ƒè½®æ•°
    :param meta_check_freq: è¿è¡Œ L2 è¯Šæ–­çš„é¢‘çŽ‡ (æ¯éš”å¤šå°‘ä¸ª batch)
    """
    optimizer = corrector.optimizer
    criterion = nn.CrossEntropyLoss()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    C_TARGET = 25.039  # æˆ‘ä»¬å¸Œæœ› E[c1] ç¨³å®šåœ¨è¿™ä¸ªå€¼é™„è¿‘

    for epoch in range(epochs):
      # é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
      data_loader = get_data_loader_func(**data_loader_params)
      num_batches = data_loader_params.get('num_batches')
      if num_batches is None:
        raise ValueError("data_loader_params å¿…é¡»åŒ…å« 'num_batches' é”®!")



    print("\n===========================================================")
    print(f"ðŸš€ å¼€å§‹å…ƒå­¦ä¹ è®­ç»ƒ (Device: {device})")
    print(f"   L2 è¯Šæ–­é¢‘çŽ‡: æ¯ {meta_check_freq} æ­¥æ£€æŸ¥ä¸€æ¬¡")
    print("===========================================================")

    for epoch in range(1, epochs + 1):
        total_loss_epoch = 0
        total_task_loss = 0

        # ðŸ’¥ ä¿®æ­£ç‚¹ï¼šåœ¨æ¯ä¸ª Epoch å¼€å§‹æ—¶é‡æ–°åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = get_data_loader_func(**data_loader_params)

        for batch_idx, (inputs, targets) in enumerate(data_loader):
            inputs, targets = inputs.to(device), targets.to(device)

            # --- L1: ä¸»å­¦ä¹ ä»»åŠ¡ä¸Žæ‹“æ‰‘æ­£åˆ™åŒ– ---
            optimizer.zero_grad()
            outputs = model(inputs) # å‰å‘ä¼ æ’­è§¦å‘æ‹“æ‰‘ç‰¹å¾è®¡ç®—

            # 1. ä»»åŠ¡æŸå¤± (å¦‚è¯­è¨€å»ºæ¨¡)
            loss_task = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))

            # 2. æ‹“æ‰‘æ­£åˆ™åŒ–æŸå¤± (åŸºäºŽ c1 å‡å€¼)
            C_TARGET = 25.039
            # æ”¶é›†æ‰€æœ‰ Layer çš„ c1 å‡å€¼
            c1_means = [layer.topology_info['first_chern_class_mean'] for layer in model.layers]
            c1_tensor = torch.tensor(c1_means, device=device) # å‡è®¾ device å˜é‡å¯ç”¨

            # ç›®æ ‡å¯¼å‘æŸå¤±: L_topo = sum((E[c1] - C_TARGET)^2)
            deviation_loss = torch.sum((c1_tensor - C_TARGET)**2)

            #{topo_loss_c1 = 0.0
            #for layer in model.layers:
                # æ‹“æ‰‘æŸå¤±: æœ€å°åŒ– c1 çš„ç»å¯¹å€¼å‡å€¼
                #topo_loss_c1 += torch.tensor(layer.topology_info['first_chern_class_mean']).abs()}

            # 3. æ€»æŸå¤± = ä»»åŠ¡æŸå¤± + Î» * æ‹“æ‰‘æŸå¤±
            total_loss = loss_task + corrector.current_lambda * deviation_loss

            total_loss.backward()
            optimizer.step()

            total_loss_epoch += total_loss.item()
            total_task_loss += loss_task.item()

            # --- L2: å…ƒå­¦ä¹ è¯Šæ–­ä¸Žä¿®æ­£ ---
            if (batch_idx + 1) % meta_check_freq == 0:

                print(f"\n[Meta-Check] Epoch {epoch}/{epochs}, Step {batch_idx + 1}")
                print(f"  å½“å‰ L1 ä»»åŠ¡æŸå¤±: {loss_task.item():.4f}")
                print(f"  å½“å‰ Î»: {corrector.current_lambda:.6f}")

                # 1. æå– L1 ç‰¹å¾
                # ç‰¹å¾æ˜¯ NumPy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (1, N_features)
                topo_features = model.collect_topo_features()
                SCALE_CORRECTION_FACTOR = 1.0 / 21.5
                for i in range(topo_features.shape[1]):
                  # åªä¿®æ­£å‡å€¼å’Œæ ‡å‡†å·®ç‰¹å¾ (ç´¢å¼• 0, 1, 3, 4)
                  if i % 5 != 2:
                     topo_features[0, i] *= SCALE_CORRECTION_FACTOR# å¿½ç•¥ c2/c1 æ¯”å€¼ (ç´¢å¼• 2)



                # 2. L2 è¯Šæ–­
                predicted_states, _ = diagnoser.predict(topo_features)
                predicted_state = predicted_states[0]

                print(f"  ðŸ§  è¯Šæ–­å™¨é¢„æµ‹çŠ¶æ€: {predicted_state} (0:æ­£å¸¸, 1:å¼‚å¸¸, 2:çº¦æŸ)")

                # 3. æå– L1 æ‹“æ‰‘ä¿¡æ¯ (åŒ…å« c1 mean, c2/c1 mean ç­‰)
                # å‡è®¾æ‚¨çš„ transformer æ¨¡å—æœ‰ä¸€ä¸ª get_topo_info æ–¹æ³•æ¥è¿”å›ž self.topology_info
                current_topo_info = model.get_current_topo_info()
                corrector.execute_correction(predicted_state, current_topo_info)



            # ç®€å•çš„è¿›åº¦æ‰“å°
            if (batch_idx + 1) % 10 == 0:
                print(f"  [Epoch {epoch}] Step {batch_idx + 1} | Loss: {total_loss.item():.4f}", end='\r')

        avg_loss = total_loss_epoch / (batch_idx + 1)
        avg_task_loss = total_task_loss / (batch_idx + 1)

        print(f"\n\n--- Epoch {epoch} æ€»ç»“ ---")
        print(f"  å¹³å‡æ€»æŸå¤±: {avg_loss:.4f}")
        print(f"  å¹³å‡ä»»åŠ¡æŸå¤±: {avg_task_loss:.4f}")
        print(f"  å¹³å‡ç¬¬ä¸€é™ˆç±» (æœ€åŽ): {model.layers[0].topology_info.get('first_chern_class_mean', 0):.6f}")


# -------------------------------------------------------
# ä¸»è¿è¡Œé€»è¾‘
# -------------------------------------------------------
if __name__ == "__main__":
    # --- é…ç½®å‚æ•° ---
    VOCAB_SIZE = 5000
    D_MODEL = 128
    NUM_LAYERS = 6
    BATCH_SIZE = 32
    SEQ_LEN = 50
    NUM_BATCHES = 200 # æ¨¡æ‹Ÿæ€»è®­ç»ƒæ­¥æ•°
    EPOCHS = 3
    INITIAL_LAMBDA = 0.005
    META_CHECK_FREQ = 20 # æ¯ 20 æ­¥è¿›è¡Œä¸€æ¬¡å…ƒå­¦ä¹ è¯Šæ–­

    # ----------------------------------------
    # I. é¢„è®­ç»ƒ L2 è¯Šæ–­å™¨ (ChernRatioClassifier)
    # ----------------------------------------
    start_time = time.time()
    # è¯Šæ–­å™¨åªéœ€è¦ä¸€ä¸ªè¾ƒå°çš„ d_model å³å¯è®­ç»ƒå…¶åˆ†ç±»é€»è¾‘
    pretrained_diagnoser = setup_and_train_diagnoser(d_model=32)
    print(f"\n>>> è¯Šæ–­å™¨é¢„è®­ç»ƒè€—æ—¶: {time.time() - start_time:.2f} ç§’ <<<")

    # ----------------------------------------
    # II. åˆå§‹åŒ– L1 æ¨¡åž‹å’Œä¿®æ­£å™¨
    # ----------------------------------------
    model = TopologyAwareTransformer(NUM_LAYERS, VOCAB_SIZE, D_MODEL)

    # æ‰“å°æ¨¡åž‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nåˆå§‹åŒ– L1 æ¨¡åž‹: TopologyAwareTransformer")
    print(f"  æ¨¡åž‹å‚æ•°æ€»æ•°: {total_params:,}")

    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    corrector = TopologicalCorrector(model, optimizer, initial_lambda=INITIAL_LAMBDA)

    # ----------------------------------------
    # III. å¯åŠ¨å…ƒå­¦ä¹ è®­ç»ƒ
    # ----------------------------------------

    # æ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
    #data_loader = get_mock_data_loader(VOCAB_SIZE, SEQ_LEN, BATCH_SIZE, NUM_BATCHES)
    loader_params = {
        'vocab_size': VOCAB_SIZE,
        'seq_len': SEQ_LEN,
        'batch_size': BATCH_SIZE,
        'num_batches': NUM_BATCHES
    }

    # å¼€å§‹è®­ç»ƒ
    train_with_meta_guidance(
        model,
        pretrained_diagnoser,
        corrector,
        get_mock_data_loader,
        loader_params,
        epochs=EPOCHS,
        meta_check_freq=META_CHECK_FREQ
    )

    print("\n===========================================================")
    print("âœ¨ æ‹“æ‰‘å…ƒå­¦ä¹ ç³»ç»Ÿè®­ç»ƒå®Œæˆã€‚")
    print("===========================================================")

    #import torch # ç¡®ä¿ torch å·²ç»è¢«å¯¼å…¥

    print("\n## ðŸš€ æœ€ç»ˆæ‹“æ‰‘ç‰¹å¾åˆ†æž (æ‰€æœ‰ Layer)")

    # ç¡®ä¿æ¨¡åž‹å¤„äºŽ CPU/è¯„ä¼°æ¨¡å¼ (å°½ç®¡è¿™é‡Œåªè¯»å–ä¿¡æ¯)
    model.eval()

    for i, layer in enumerate(model.layers):
        # å‡è®¾ model.layers[i] æ˜¯æ‚¨çš„ TopologyAwareTransformerLayer å®žä¾‹
        # å¹¶ä¸” info å­—å…¸è¢«æ­£ç¡®å­˜å‚¨åœ¨ layer.topology_info ä¸­
        info = layer.topology_info

        # ä½¿ç”¨ .get() ç¡®ä¿é”®ä¸å­˜åœ¨æ—¶ç¨‹åºä¸ä¼šå´©æºƒ
        c1_mean = info.get('first_chern_class_mean', float('nan'))
        c2_c1_mean = info.get('chern_ratio_mean', float('nan'))
        c1_std = info.get('first_chern_class_std', float('nan'))

        # å¦‚æžœ mean/std æ˜¯ torch.Tensorï¼Œéœ€è¦è½¬æ¢ä¸º float
        if isinstance(c1_mean, torch.Tensor):
            c1_mean = c1_mean.item()
        if isinstance(c2_c1_mean, torch.Tensor):
            c2_c1_mean = c2_c1_mean.item()
        if isinstance(c1_std, torch.Tensor):
            c1_std = c1_std.item()

        print(f"Layer {i}:")
        print(f"  c1 Mean: {c1_mean:.4f}")
        print(f"  c2/c1 Mean: {c2_c1_mean:.4f}")
        print(f"  c1 Std: {c1_std:.4f}")
    # =======================================================
