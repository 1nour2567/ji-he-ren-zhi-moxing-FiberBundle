import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import math
import time
from collections import deque
import warnings
warnings.filterwarnings('ignore')

class CognitiveFiberBundle(nn.Module):
    """è®¤çŸ¥çº¤ç»´ä¸›æ¨¡å‹ - AGIå®‰å…¨ç›‘æ§æ ¸å¿ƒ"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.k_neighbors = k_neighbors
        
        # åŠ¨æ€è”ç»œå½¢å¼ç½‘ç»œ
        self.connection_network = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Linear(d_model * 2, d_model * d_model)
        )
        
        # æ›²ç‡è®¡ç®—ç½‘ç»œ
        self.curvature_network = nn.Sequential(
            nn.Linear(d_model * d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model * d_model)
        )
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.Dropout(0.1)
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.Dropout(0.1)
            )
        ])
        
        # ç¼“å­˜
        self.activation_cache = deque(maxlen=100)
        self.topology_cache = deque(maxlen=100)
        self.monitoring_enabled = True
        
    def compute_connection_form(self, x):
        batch_size, n, d = x.shape
        x_flat = x.reshape(-1, d)
        connection_params = self.connection_network(x_flat)
        connection = connection_params.reshape(-1, d, d)
        connection_sym = 0.5 * (connection + connection.transpose(1, 2))
        connection_anti = 0.5 * (connection - connection.transpose(1, 2))
        final_connection = connection_sym * 0.3 + connection_anti * 0.7
        return final_connection.reshape(batch_size, n, d, d)
    
    def compute_curvature_form(self, connection):
        batch_size, n, d, _ = connection.shape
        connection_flat = connection.reshape(-1, d, d)
        A_wedge_A = torch.bmm(connection_flat, connection_flat) - \
                   torch.bmm(connection_flat.transpose(1, 2), connection_flat)
        if n > 1:
            conn = connection.reshape(batch_size, n, d, d)
            dA = torch.zeros_like(conn)
            for i in range(n-1):
                dA[:, i] = conn[:, i+1] - conn[:, i]
            dA[:, -1] = conn[:, -1] - conn[:, -2]
            dA_flat = dA.reshape(-1, d, d)
        else:
            dA_flat = torch.zeros_like(connection_flat)
        curvature = dA_flat + A_wedge_A
        curvature_params = curvature.reshape(-1, d*d)
        enhanced_curvature = self.curvature_network(curvature_params)
        return enhanced_curvature.reshape(batch_size, n, d, d)
    
    def compute_chern_classes(self, curvature):
        batch_size, n, d, _ = curvature.shape
        results = {k: [] for k in ['first_chern', 'second_chern', 'chern_ratio',
                                   'curvature_norm', 'curvature_trace', 'euler_characteristic']}
        curvature_flat = curvature.reshape(-1, d, d)
        for i in range(batch_size * n):
            F = curvature_flat[i]
            trace_F = torch.trace(F)
            first_chern = (1j / (2 * math.pi)) * trace_F
            F_sq = torch.mm(F, F)
            trace_F_sq = torch.trace(F_sq)
            second_chern = (1 / (8 * math.pi**2)) * (trace_F_sq - trace_F**2)
            chern_ratio = abs(second_chern) / (abs(first_chern) + 1e-8)
            results['first_chern'].append(first_chern.real)
            results['second_chern'].append(second_chern.real)
            results['chern_ratio'].append(chern_ratio)
            results['curvature_norm'].append(torch.norm(F).item())
            results['curvature_trace'].append(trace_F.real.item())
            euler_approx = 2 * first_chern.real - second_chern.real
            results['euler_characteristic'].append(euler_approx)
        for key in results:
            # âœ… ç¡®ä¿è¾“å‡ºä¸º float32 å¼ é‡
            results[key] = torch.tensor(results[key], dtype=torch.float32).reshape(batch_size, n)
        return results
    
    def compute_topological_invariants(self, x):
        connection = self.compute_connection_form(x)
        curvature = self.compute_curvature_form(connection)
        chern_classes = self.compute_chern_classes(curvature)
        batch_size, n, d = x.shape
        curvature_eigenvalues = []
        curvature_flat = curvature.reshape(-1, d, d)
        for i in range(batch_size * n):
            F = curvature_flat[i].detach().cpu().numpy()
            eigvals = np.linalg.eigvals(F)
            curvature_eigenvalues.append(eigvals)
        zero_curvature_dims = [np.sum(np.abs(eig) < 1e-6) for eig in curvature_eigenvalues]
        persistence = [np.max(np.real(eig)) - np.min(np.real(eig)) for eig in curvature_eigenvalues]
        # âœ… å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ float32
        chern_classes['betti_approx'] = torch.tensor(zero_curvature_dims, dtype=torch.float32).reshape(batch_size, n)
        chern_classes['persistence'] = torch.tensor(persistence, dtype=torch.float32).reshape(batch_size, n)
        return chern_classes
    
    def forward(self, x):
        batch_size, n, d = x.shape
        if self.monitoring_enabled:
            self.activation_cache.append({'type': 'input', 'data': x.detach().cpu().numpy(), 'timestamp': time.time()})
        input_topology = self.compute_topological_invariants(x)
        current_output = x
        layer_topologies = {}
        for i, layer in enumerate(self.geometric_layers):
            flat_in = current_output.reshape(-1, d)
            flat_out = layer(flat_in)
            current_output = flat_out.reshape(batch_size, n, d)
            layer_topology = self.compute_topological_invariants(current_output)
            layer_topologies[f'layer_{i+1}'] = layer_topology
            if self.monitoring_enabled:
                self.activation_cache.append({
                    'type': f'layer_{i+1}',
                    'data': current_output.detach().cpu().numpy(),
                    'timestamp': time.time()
                })
        output_topology = self.compute_topological_invariants(current_output)
        if self.monitoring_enabled:
            self.topology_cache.append({
                'input': input_topology,
                'layers': layer_topologies,
                'output': output_topology,
                'timestamp': time.time()
            })
        return {'output': current_output, 'topology': {
            'input': input_topology,
            'layers': layer_topologies,
            'output': output_topology
        }}

class TopologicalAnomalyDetector:
    def __init__(self):
        self.scaler = StandardScaler()
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.fitted = False
        self.feature_history = deque(maxlen=1000)
        self.label_history = deque(maxlen=1000)
        
    def extract_topological_features(self, topology_data):
        output_topo = topology_data['output']
        features = [
            torch.mean(output_topo['second_chern']).item(),
            torch.std(output_topo['second_chern']).item(),
            torch.mean(output_topo['curvature_norm']).item(),
            torch.std(output_topo['curvature_norm']).item(),
            torch.mean(output_topo['chern_ratio']).item(),
            torch.std(output_topo['chern_ratio']).item(),
            torch.mean(output_topo['euler_characteristic']).item(),
            torch.mean(output_topo['betti_approx']).item(),      # âœ… å·²ç¡®ä¿ä¸º float
            torch.mean(output_topo['persistence']).item(),       # âœ… å·²ç¡®ä¿ä¸º float
            torch.mean(output_topo['curvature_trace']).item()
        ]
        for layer_key in ['layer_1', 'layer_2']:
            if layer_key in topology_data['layers']:
                layer_topo = topology_data['layers'][layer_key]
                features.extend([
                    torch.mean(layer_topo['second_chern']).item(),
                    torch.mean(layer_topo['chern_ratio']).item()
                ])
        return np.array(features)
    
    def fit(self, topology_data_list):
        features_list = [self.extract_topological_features(td) for td in topology_data_list]
        features_array = np.array(features_list)
        self.scaler.fit(features_array)
        self.anomaly_detector.fit(self.scaler.transform(features_array))
        self.fitted = True
        return self
    
    def predict(self, topology_data):
        if not self.fitted:
            raise ValueError("æ£€æµ‹å™¨å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        features = self.extract_topological_features(topology_data)
        scaled = self.scaler.transform([features])
        pred = self.anomaly_detector.predict(scaled)[0]
        score = self.anomaly_detector.score_samples(scaled)[0]
        return pred == -1, score

class TopologicalAutoRepair:
    def __init__(self):
        self.repair_history = deque(maxlen=100)
        
    def detect_topological_anomaly(self, topology_data):
        output_topo = topology_data['output']
        second_chern_mean = torch.mean(output_topo['second_chern']).item()
        chern_ratio_mean = torch.mean(output_topo['chern_ratio']).item()
        curvature_norm_mean = torch.mean(output_topo['curvature_norm']).item()
        betti_numbers = torch.mean(output_topo['betti_approx']).item()
        anomalies = []
        if abs(second_chern_mean) > 0.1:
            anomalies.append({'type': 'second_chern_anomaly', 'severity': 'high' if abs(second_chern_mean) > 0.2 else 'medium', 'value': second_chern_mean})
        if chern_ratio_mean > 2.0:
            anomalies.append({'type': 'chern_ratio_anomaly', 'severity': 'high' if chern_ratio_mean > 5.0 else 'medium', 'value': chern_ratio_mean})
        if curvature_norm_mean > 2.0:
            anomalies.append({'type': 'curvature_norm_anomaly', 'severity': 'high' if curvature_norm_mean > 5.0 else 'medium', 'value': curvature_norm_mean})
        if betti_numbers < 0.5:
            anomalies.append({'type': 'topological_degeneration', 'severity': 'high', 'value': betti_numbers})
        return anomalies
    
    def generate_repair_strategy(self, anomalies):
        if not anomalies:
            return {'action': 'none', 'strategy': 'system_stable', 'confidence': 1.0, 'repair_instructions': []}
        repair_instructions = []
        for anomaly in anomalies:
            if anomaly['type'] == 'second_chern_anomaly':
                repair_instructions.append({'type': 'curvature_regularization', 'target': 'second_chern', 'direction': 'reduce' if anomaly['value'] > 0 else 'increase'})
            elif anomaly['type'] == 'chern_ratio_anomaly':
                repair_instructions.append({'type': 'connection_form_adjustment', 'target': 'chern_ratio', 'direction': 'stabilize'})
            elif anomaly['type'] == 'curvature_norm_anomaly':
                repair_instructions.append({'type': 'topological_smoothing', 'target': 'curvature_norm', 'direction': 'reduce'})
            elif anomaly['type'] == 'topological_degeneration':
                repair_instructions.append({'type': 'topological_reconstruction', 'target': 'betti_numbers', 'direction': 'increase'})
        severity_weights = {'high': 0.9, 'medium': 0.7, 'low': 0.5}
        total_weight = sum(severity_weights.get(a['severity'], 0.5) for a in anomalies)
        confidence = max(0.5, 1.0 - (total_weight / len(anomalies) * 0.3))
        return {'action': 'repair', 'strategy': 'multi_stage_repair', 'confidence': confidence, 'repair_instructions': repair_instructions, 'anomaly_count': len(anomalies)}
    
    def apply_repair(self, model, repair_strategy):
        if repair_strategy['action'] == 'none':
            return model, True
        success_count = 0
        total_attempts = len(repair_strategy['repair_instructions'])
        for instruction in repair_strategy['repair_instructions']:
            try:
                if instruction['type'] == 'curvature_regularization':
                    with torch.no_grad():
                        for param in model.curvature_network.parameters():
                            param.data *= 0.95
                    success_count += 1
                elif instruction['type'] == 'connection_form_adjustment':
                    with torch.no_grad():
                        for param in model.connection_network.parameters():
                            noise = torch.randn_like(param) * 0.01
                            param.data += noise
                    success_count += 1
                elif instruction['type'] == 'topological_smoothing':
                    with torch.no_grad():
                        for layer in model.geometric_layers:
                            for param in layer.parameters():
                                if param.requires_grad:
                                    param.data *= 0.98
                    success_count += 1
            except Exception as e:
                continue
        repair_success = success_count >= max(1, total_attempts // 2)
        return model, repair_success

class AGISafetyMonitor:
    def __init__(self):
        self.topology_detector = TopologicalAnomalyDetector()
        self.auto_repair = TopologicalAutoRepair()
        self.system_status = {
            'security_level': 'normal',
            'anomaly_count': 0,
            'repair_count': 0,
            'topology_health': 'excellent',
            'last_alert': None,
            'system_uptime': 0
        }
        self.detection_history = deque(maxlen=1000)
        self.repair_history = deque(maxlen=100)
        self.consecutive_normal = 0  # âœ… æ–°å¢ï¼šè¿ç»­æ­£å¸¸è®¡æ•°å™¨
        self.anomaly_threshold = 0.15
        self.critical_threshold = 0.8
        
    def initialize_monitoring(self, model, num_samples=100):
        print("ğŸ”§ åˆå§‹åŒ–AGIå®‰å…¨ç›‘æ§ç³»ç»Ÿ...")
        training_data = []
        batch_size, vocab_size, d_model = 1, 32, 32
        for i in range(num_samples):
            inputs = torch.randn(batch_size, vocab_size, d_model) * 0.5
            result = model(inputs)
            training_data.append(result['topology'])
        self.topology_detector.fit(training_data)
        print(f"âœ… å®‰å…¨ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ {num_samples} ä¸ªæ­£å¸¸æ ·æœ¬è®­ç»ƒ")
        return self
    
    def monitor_and_protect(self, model, inputs):
        result = model(inputs)
        topology_data = result['topology']
        is_anomaly, anomaly_score = self.topology_detector.predict(topology_data)
        self.system_status['system_uptime'] += 1
        
        # âœ… æ›´æ–°è¿ç»­æ­£å¸¸è®¡æ•°å™¨
        if not is_anomaly:
            self.consecutive_normal += 1
        else:
            self.consecutive_normal = 0
            self.system_status['anomaly_count'] += 1
            
            if anomaly_score < -self.critical_threshold:
                self.system_status['security_level'] = 'critical'
                self.system_status['topology_health'] = 'critical'
            elif anomaly_score < -self.anomaly_threshold:
                self.system_status['security_level'] = 'warning'
                if self.system_status['topology_health'] == 'excellent':
                    self.system_status['topology_health'] = 'good'
        
        # è‡ªåŠ¨ä¿®å¤é€»è¾‘
        repair_success = False
        if is_anomaly:
            anomalies = self.auto_repair.detect_topological_anomaly(topology_data)
            if anomalies:
                repair_strategy = self.auto_repair.generate_repair_strategy(anomalies)
                model, repair_success = self.auto_repair.apply_repair(model, repair_strategy)
                self.system_status['repair_count'] += 1
                
                # âœ… å…³é”®ä¿®å¤ï¼šä¿®å¤åé‡æ–°è¯„ä¼°å¹¶è¦†ç›–å½“å‰ç»“æœ
                if repair_success:
                    result = model(inputs)
                    topology_data = result['topology']
                    is_anomaly, anomaly_score = self.topology_detector.predict(topology_data)
                    
                    if not is_anomaly:
                        self.consecutive_normal += 1
        
        # âœ… æ ¹æ®è¿ç»­æ­£å¸¸æ¬¡æ•°æ¢å¤çŠ¶æ€
        if self.consecutive_normal >= 5:
            self.system_status['security_level'] = 'normal'
            self.system_status['topology_health'] = 'excellent'
        
        # è®°å½•å†å²
        detection_record = {
            'timestamp': time.time(),
            'is_anomaly': is_anomaly,
            'anomaly_score': anomaly_score,
            'security_level': self.system_status['security_level'],
            'topology_health': self.system_status['topology_health']
        }
        self.detection_history.append(detection_record)
        
        if is_anomaly and repair_success:
            self.repair_history.append({
                'timestamp': time.time(),
                'success': repair_success,
                'anomaly_score_before': anomaly_score
            })
        
        return {
            'model_output': result['output'],
            'topology_data': topology_data,
            'is_anomaly': is_anomaly,
            'anomaly_score': anomaly_score,
            'repair_success': repair_success,
            'system_status': self.system_status.copy()
        }
    
    def get_security_report(self):
        if len(self.detection_history) == 0:
            return "ç³»ç»Ÿå°šæœªå¼€å§‹ç›‘æ§"
        recent_detections = list(self.detection_history)[-50:]
        anomaly_rate = sum(1 for d in recent_detections if d['is_anomaly']) / len(recent_detections)
        report = f"""
ğŸ›¡ï¸ AGIå®‰å…¨ç›‘æ§æŠ¥å‘Š
==================
ğŸ“Š ç³»ç»ŸçŠ¶æ€:
   â€¢ å®‰å…¨ç­‰çº§: {self.system_status['security_level']}
   â€¢ æ‹“æ‰‘å¥åº·: {self.system_status['topology_health']}
   â€¢ è¿è¡Œæ—¶é—´: {self.system_status['system_uptime']} æ¬¡ç›‘æ§
   â€¢ å¼‚å¸¸è®¡æ•°: {self.system_status['anomaly_count']}
   â€¢ ä¿®å¤è®¡æ•°: {self.system_status['repair_count']}

ğŸ“ˆ æœ€è¿‘ç»Ÿè®¡:
   â€¢ å¼‚å¸¸ç‡: {anomaly_rate*100:.2f}% (æœ€è¿‘50æ¬¡)
   â€¢ æ£€æµ‹æ€»æ•°: {len(self.detection_history)}
   â€¢ ä¿®å¤å†å²: {len(self.repair_history)} æ¬¡

ğŸ” é£é™©è¯„ä¼°:
"""
        if anomaly_rate > 0.3:
            report += "   âš ï¸ é«˜é£é™©: å¼‚å¸¸ç‡ > 30%\n"
        elif anomaly_rate > 0.1:
            report += "   ğŸŸ¡ ä¸­ç­‰é£é™©: å¼‚å¸¸ç‡ 10-30%\n"
        else:
            report += "   âœ… ä½é£é™©: å¼‚å¸¸ç‡ < 10%\n"
        
        if len(self.repair_history) > 0:
            recent_repair = self.repair_history[-1]
            report += f"   ğŸ› ï¸ æœ€è¿‘ä¿®å¤: {recent_repair['success']}\n"
        return report

def simulate_agi_safety_monitoring():
    print("ğŸ›¡ï¸ å¯åŠ¨å®Œæ•´AGIå®‰å…¨ç›‘æ§å¥—ä»¶...")
    print("=" * 60)
    vocab_size = 32
    d_model = 32
    model = CognitiveFiberBundle(vocab_size, d_model=d_model)
    safety_monitor = AGISafetyMonitor()
    safety_monitor.initialize_monitoring(model, num_samples=50)
    print("\nğŸš€ å¼€å§‹å®‰å…¨ç›‘æ§æ¨¡æ‹Ÿ...")
    print("æ¨¡æ‹Ÿåœºæ™¯: æ­£å¸¸è¾“å…¥ â†’ å¼‚å¸¸æ³¨å…¥ â†’ çº¦æŸè¿å â†’ ç³»ç»Ÿä¿®å¤ â†’ æ¢å¤æ­£å¸¸")
    for step in range(100):
        if step < 20:
            inputs = torch.randn(1, vocab_size, d_model) * 0.5
            scenario = "æ­£å¸¸è¾“å…¥"
        elif step < 40:
            inputs = torch.randn(1, vocab_size, d_model) * 0.5
            inputs[0, step % vocab_size, :] = torch.ones(d_model) * 15.0
            scenario = "å¼‚å¸¸æ³¨å…¥"
        elif step < 60:
            inputs = torch.randn(1, vocab_size, d_model) * 0.5
            inputs[0, :, :] = torch.ones(vocab_size, d_model) * (step % 8)
            scenario = "çº¦æŸè¿å"
        elif step < 80:
            inputs = torch.randn(1, vocab_size, d_model) * 0.3
            scenario = "ä¿®å¤æœŸ"
        else:
            inputs = torch.randn(1, vocab_size, d_model) * 0.5
            scenario = "æ¢å¤æ­£å¸¸"
        result = safety_monitor.monitor_and_protect(model, inputs)
        if step % 10 == 0:
            status = result['system_status']
            print(f"â±ï¸ æ­¥éª¤ {step:3d} | åœºæ™¯: {scenario:8s} | "
                  f"å®‰å…¨: {status['security_level']:8s} | "
                  f"å¥åº·: {status['topology_health']:8s} | "
                  f"å¼‚å¸¸: {result['is_anomaly']}")
    print("\n" + "=" * 60)
    print(safety_monitor.get_security_report())
    print(f"\nğŸ¯ ä¿®å¤ç‰ˆæ”¹è¿›è¯´æ˜:")
    print(f"  â€¢ æ‰€æœ‰æ‹“æ‰‘ç‰¹å¾ç¡®ä¿ä¸º float32 ç±»å‹")
    print(f"  â€¢ ä¿®å¤åç«‹å³é‡æ–°è¯„ä¼°å¹¶æ›´æ–°çŠ¶æ€")
    print(f"  â€¢ å¼•å…¥è¿ç»­æ­£å¸¸è®¡æ•°å™¨å®ç°çŠ¶æ€æ¢å¤")
    print(f"  â€¢ ä¿®å¤é€»è¾‘ä¸ä¸»ç›‘æ§å¾ªç¯å®Œå…¨è€¦åˆ")
    return model, safety_monitor

if __name__ == "__main__":
    model, safety_monitor = simulate_agi_safety_monitoring()
