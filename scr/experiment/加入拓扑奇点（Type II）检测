import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse.csgraph import connected_components

class SparseFiberBundle(nn.Module):
    """ç¨€ç–çº¤ç»´ä¸›è¿‘ä¼¼ç®—æ³• - æ”¯æŒæ‹“æ‰‘ç»“æ„åˆ†æ"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.k_neighbors = k_neighbors
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ç”¨äºå¥‡ç‚¹æ£€æµ‹
        self.activations = {}
        
        # æ‹“æ‰‘åˆ†æç›¸å…³
        self.topology_history = []
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            )
        ])
    
    def compute_connectivity_matrix(self, x):
        """è®¡ç®—æ¿€æ´»æ¨¡å¼çš„è¿æ¥æ€§çŸ©é˜µ - æ‹“æ‰‘åˆ†æåŸºç¡€"""
        batch_size, n, d = x.shape
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—è¿æ¥æ€§
        x_flat = x.view(batch_size * n, d)
        similarity = torch.mm(x_flat, x_flat.t()) / (
            torch.norm(x_flat, dim=1, keepdim=True) * torch.norm(x_flat, dim=1, keepdim=True).t() + 1e-8
        )
        # åªä¿ç•™æ­£ç›¸å…³è¿æ¥
        connectivity = torch.clamp(similarity, min=0.0)
        return connectivity
    
    def compute_topological_features(self, x):
        """è®¡ç®—æ‹“æ‰‘ç‰¹å¾ - ç”¨äºæ£€æµ‹Type IIå¥‡ç‚¹"""
        connectivity = self.compute_connectivity_matrix(x)
        batch_size = x.shape[0]
        
        # è®¡ç®—è¿é€šåˆ†é‡æ•°é‡ï¼ˆæ‹“æ‰‘ä¸å˜é‡ï¼‰
        connectivity_np = connectivity.detach().cpu().numpy()
        n_components, labels = connected_components(
            csgraph=connectivity_np, 
            directed=False, 
            return_labels=True
        )
        
        return {
            'n_components': n_components,
            'connectivity': connectivity,
            'labels': torch.tensor(labels).to(x.device),
            'density': torch.mean(connectivity).item()
        }
    
    def forward(self, x):
        """x: [batch_size, vocab_size, d_model]"""
        batch_size, n, d = x.shape
        x_flat = x.view(-1, d)
        
        # ä¿å­˜è¾“å…¥ç”¨äºå¥‡ç‚¹æ£€æµ‹
        self.activations['input'] = x.clone()
        self.activations['input_topology'] = self.compute_topological_features(x)
        
        # åº”ç”¨å‡ ä½•å˜æ¢
        for i, layer in enumerate(self.geometric_layers):
            x_flat = layer(x_flat)
            layer_output = x_flat.view(batch_size, n, d)
            self.activations[f'layer{i+1}'] = layer_output.clone()
            self.activations[f'layer{i+1}_topology'] = self.compute_topological_features(layer_output)
        
        x = x_flat.view(batch_size, n, d)
        self.activations['output'] = x.clone()
        self.activations['output_topology'] = self.compute_topological_features(x)
        return x

class SingularityDrivenLearner:
    """å¥‡ç‚¹é©±åŠ¨å­¦ä¹ ç®—æ³• - æ”¯æŒType Iå’ŒType IIå¥‡ç‚¹"""
    def __init__(self, model):
        self.model = model
        self.singularity_history = []
        self.singularity_threshold = 2.0  # é€»è¾‘å¥‡ç‚¹é˜ˆå€¼
        self.topology_change_threshold = 0.5  # æ‹“æ‰‘å˜åŒ–é˜ˆå€¼
    
    def detect_type_I_singularities(self):
        """æ£€æµ‹Type Iå¥‡ç‚¹ï¼ˆé€»è¾‘å¥‡ç‚¹ï¼‰"""
        detected_singularities = []
        
        for layer_name, activation in self.model.activations.items():
            if 'topology' not in layer_name:  # åªæ£€æµ‹æ¿€æ´»å€¼
                max_abs = torch.max(torch.abs(activation))
                if max_abs > self.singularity_threshold:
                    positions = torch.where(torch.abs(activation) > self.singularity_threshold)
                    detected_singularities.append({
                        'type': 'Type_I',
                        'layer': layer_name,
                        'detected': True,
                        'value': max_abs.item(),
                        'positions': positions,
                        'count': len(positions[0])
                    })
        
        return detected_singularities
    
    def detect_type_II_singularities(self):
        """æ£€æµ‹Type IIå¥‡ç‚¹ï¼ˆæ‹“æ‰‘å¥‡ç‚¹ï¼‰"""
        detected_singularities = []
        
        # æ¯”è¾ƒç›¸é‚»å±‚çš„æ‹“æ‰‘ç»“æ„å˜åŒ–
        topology_keys = [k for k in self.model.activations.keys() if 'topology' in k]
        
        for i in range(len(topology_keys)-1):
            current_key = topology_keys[i]
            next_key = topology_keys[i+1]
            
            current_topo = self.model.activations[current_key]
            next_topo = self.model.activations[next_key]
            
            # æ£€æµ‹è¿é€šåˆ†é‡æ•°é‡å˜åŒ–ï¼ˆæ‹“æ‰‘çªå˜ï¼‰
            comp_change = abs(current_topo['n_components'] - next_topo['n_components'])
            
            # æ£€æµ‹è¿æ¥å¯†åº¦å˜åŒ–ï¼ˆæ‹“æ‰‘ç»“æ„å˜åŒ–ï¼‰
            density_change = abs(current_topo['density'] - next_topo['density'])
            
            if comp_change > 0 or density_change > self.topology_change_threshold:
                detected_singularities.append({
                    'type': 'Type_II',
                    'layer_transition': f"{current_key} -> {next_key}",
                    'detected': True,
                    'n_components_change': comp_change,
                    'density_change': density_change,
                    'current_components': current_topo['n_components'],
                    'next_components': next_topo['n_components']
                })
        
        return detected_singularities
    
    def apply_repair_to_tensor(self, tensor):
        """å¯¹å¼ é‡åº”ç”¨ä¿®å¤ç­–ç•¥"""
        # ç­–ç•¥1: Clampåˆ°åˆç†èŒƒå›´
        repaired = torch.clamp(tensor, -1.0, 1.0)
        return repaired
    
    def apply_topological_repair(self, tensor, topology_info):
        """åº”ç”¨æ‹“æ‰‘ä¿®å¤ç­–ç•¥"""
        # ç®€åŒ–ç‰ˆæ‹“æ‰‘ä¿®å¤ï¼šé€šè¿‡å¹³æ»‘è¿æ¥æ€§æ¥ç¨³å®šæ‹“æ‰‘
        connectivity = topology_info['connectivity']
        
        # ä½¿ç”¨è¿æ¥æ€§ä¿¡æ¯æ¥å¹³æ»‘å¼ é‡ï¼ˆä¿æŒæ‹“æ‰‘ä¸€è‡´æ€§ï¼‰
        # è¿™é‡Œç”¨ä¸€ä¸ªç®€å•çš„å¹³å‡æ± åŒ–æ¥æ¨¡æ‹Ÿæ‹“æ‰‘çº¦æŸ
        batch_size, n, d = tensor.shape
        tensor_flat = tensor.view(batch_size * n, d)
        
        # åŸºäºè¿æ¥æ€§çš„å¹³æ»‘
        weighted_sum = torch.mm(connectivity, tensor_flat)
        weights = torch.sum(connectivity, dim=1, keepdim=True) + 1e-8
        smoothed = weighted_sum / weights
        return smoothed.view(batch_size, n, d)
    
    def train_step_with_proactive_repair(self, batch):
        """ä¸»åŠ¨ä¿®å¤æ¨¡å¼ï¼šæ£€æµ‹å¹¶ä¿®å¤æ‰€æœ‰ç±»å‹å¥‡ç‚¹"""
        # 1. å‰å‘ä¼ æ’­
        outputs = self.model(batch)
        
        # 2. æ£€æµ‹Type Iå¥‡ç‚¹ï¼ˆé€»è¾‘å¥‡ç‚¹ï¼‰
        type_I_singularities = self.detect_type_I_singularities()
        
        # 3. æ£€æµ‹Type IIå¥‡ç‚¹ï¼ˆæ‹“æ‰‘å¥‡ç‚¹ï¼‰
        type_II_singularities = self.detect_type_II_singularities()
        
        # 4. åˆå¹¶æ‰€æœ‰å¥‡ç‚¹
        all_singularities = type_I_singularities + type_II_singularities
        
        # 5. åº”ç”¨ä¿®å¤
        repaired_outputs = outputs
        if type_I_singularities:
            # ä¿®å¤é€»è¾‘å¥‡ç‚¹
            repaired_outputs = self.apply_repair_to_tensor(repaired_outputs)
        
        if type_II_singularities:
            # ä¿®å¤æ‹“æ‰‘å¥‡ç‚¹ï¼ˆä½¿ç”¨è¾“å‡ºå±‚çš„æ‹“æ‰‘ä¿¡æ¯ï¼‰
            output_topo = self.model.activations['output_topology']
            repaired_outputs = self.apply_topological_repair(repaired_outputs, output_topo)
        
        # 6. è®°å½•å¥‡ç‚¹å†å²
        self.singularity_history.extend(all_singularities)
        
        # 7. è®¡ç®—æŸå¤±
        loss = torch.mean((repaired_outputs - batch) ** 2)
        
        # 8. å¥‡ç‚¹å¥–åŠ±æœºåˆ¶
        if all_singularities:
            reward = 0.1 * len(all_singularities)
            loss = loss - reward
        
        return repaired_outputs, loss, all_singularities

def visualize_comprehensive_singularity_detection():
    """å¯è§†åŒ–ç»¼åˆå¥‡ç‚¹æ£€æµ‹"""
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 1
    vocab_size = 20
    d_model = 64
    
    # 1. åˆ›å»ºæ¨¡å‹
    model = SparseFiberBundle(vocab_size, d_model=d_model)
    learner = SingularityDrivenLearner(model)
    
    # 2. åˆ›å»ºåŒ…å«å¼‚å¸¸å€¼çš„è¾“å…¥
    inputs = torch.randn(batch_size, vocab_size, d_model)
    
    # åˆ¶é€ å‡ ç§å¼‚å¸¸ï¼š
    # é€»è¾‘å¼‚å¸¸ï¼ˆType Iï¼‰
    inputs[0, 0, 0] = 10.0      # é€»è¾‘å¼‚å¸¸å€¼1
    inputs[0, 5, 32] = -15.0    # é€»è¾‘å¼‚å¸¸å€¼2
    
    # æ‹“æ‰‘å¼‚å¸¸ï¼šé€šè¿‡æ”¹å˜ç‰¹å®šä½ç½®çš„å€¼æ¥å½±å“è¿æ¥æ€§
    inputs[0, 10, :] = 20.0     # é«˜æ¿€æ´»å€¼å½±å“æ‹“æ‰‘
    inputs[0, 15, :] = -20.0    # é«˜æ¿€æ´»å€¼å½±å“æ‹“æ‰‘
    
    print("ğŸ” åˆ¶é€ çš„å¼‚å¸¸å€¼:")
    print(f"  é€»è¾‘å¼‚å¸¸: ä½ç½®(0,0,0)={inputs[0, 0, 0].item():.2f}, ä½ç½®(0,5,32)={inputs[0, 5, 32].item():.2f}")
    print(f"  æ‹“æ‰‘å¼‚å¸¸: ä½ç½®(0,10,:)å’Œ(0,15,:)ï¼ˆé«˜æ¿€æ´»å€¼ï¼‰")
    
    # 3. ç»¼åˆä¿®å¤è®­ç»ƒ
    outputs, loss, detected_singularities = learner.train_step_with_proactive_repair(inputs)
    
    # 4. å¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    # å­å›¾1: è¾“å…¥æ•°æ®åˆ†å¸ƒ
    input_flat = inputs[0].view(-1).detach().numpy()
    axes[0].hist(input_flat, bins=50, alpha=0.7, color='red', edgecolor='black')
    axes[0].set_title('è¾“å…¥æ•°æ®åˆ†å¸ƒ (å«å¼‚å¸¸å€¼)', fontsize=12)
    axes[0].set_xlabel('å€¼')
    axes[0].set_ylabel('é¢‘æ¬¡')
    axes[0].axvline(x=learner.singularity_threshold, color='orange', linestyle='--', label='é€»è¾‘é˜ˆå€¼')
    axes[0].axvline(x=-learner.singularity_threshold, color='orange', linestyle='--')
    axes[0].legend()
    
    # å­å›¾2: è¾“å‡ºæ•°æ®åˆ†å¸ƒ
    output_flat = outputs[0].view(-1).detach().numpy()
    axes[1].hist(output_flat, bins=50, alpha=0.7, color='blue', edgecolor='black')
    axes[1].set_title('è¾“å‡ºæ•°æ®åˆ†å¸ƒ (ä¿®å¤å)', fontsize=12)
    axes[1].set_xlabel('å€¼')
    axes[1].set_ylabel('é¢‘æ¬¡')
    axes[1].axvline(x=1, color='green', linestyle='--', label='ä¿®å¤è¾¹ç•Œ')
    axes[1].axvline(x=-1, color='green', linestyle='--')
    axes[1].legend()
    
    # å­å›¾3: æ‹“æ‰‘ç»“æ„å˜åŒ–
    topo_info = model.activations['input_topology']
    next_topo_info = model.activations['output_topology']
    
    axes[2].bar(['è¾“å…¥å±‚', 'è¾“å‡ºå±‚'], 
                [topo_info['n_components'], next_topo_info['n_components']], 
                color=['red', 'blue'], alpha=0.7)
    axes[2].set_title(f'æ‹“æ‰‘è¿é€šåˆ†é‡æ•°\nè¾“å…¥: {topo_info["n_components"]}, è¾“å‡º: {next_topo_info["n_components"]}', 
                     fontsize=12)
    axes[2].set_ylabel('è¿é€šåˆ†é‡æ•°')
    
    # å­å›¾4: è¿æ¥å¯†åº¦å˜åŒ–
    axes[3].bar(['è¾“å…¥å±‚', 'è¾“å‡ºå±‚'], 
                [topo_info['density'], next_topo_info['density']], 
                color=['red', 'blue'], alpha=0.7)
    axes[3].set_title(f'è¿æ¥å¯†åº¦\nè¾“å…¥: {topo_info["density"]:.3f}, è¾“å‡º: {next_topo_info["density"]:.3f}', 
                     fontsize=12)
    axes[3].set_ylabel('è¿æ¥å¯†åº¦')
    
    # å­å›¾5: ä¿®å¤å‰åå¯¹æ¯”
    n_compare = min(100, input_flat.shape[0])
    x_pos = np.arange(n_compare)
    
    axes[4].plot(x_pos, input_flat[:n_compare], 'ro-', alpha=0.6, label='ä¿®å¤å‰', markersize=4)
    axes[4].plot(x_pos, output_flat[:n_compare], 'bo-', alpha=0.6, label='ä¿®å¤å', markersize=4)
    axes[4].set_title('ä¿®å¤å‰åå¯¹æ¯” (å‰100ä¸ªå€¼)', fontsize=12)
    axes[4].set_xlabel('å…ƒç´ ç´¢å¼•')
    axes[4].set_ylabel('å€¼')
    axes[4].legend()
    axes[4].grid(True, alpha=0.3)
    
    # å­å›¾6: å¥‡ç‚¹ç±»å‹ç»Ÿè®¡
    type_I_count = len([s for s in detected_singularities if s['type'] == 'Type_I'])
    type_II_count = len([s for s in detected_singularities if s['type'] == 'Type_II'])
    
    if type_I_count > 0 or type_II_count > 0:
        bars = axes[5].bar(['Type I (é€»è¾‘)', 'Type II (æ‹“æ‰‘)'], 
                          [type_I_count, type_II_count], 
                          color=['orange', 'purple'], alpha=0.7)
        axes[5].set_title(f'æ£€æµ‹åˆ°çš„å¥‡ç‚¹ç±»å‹\næ€»è®¡: {len(detected_singularities)}ä¸ª', fontsize=12)
        axes[5].set_ylabel('æ•°é‡')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, [type_I_count, type_II_count]):
            axes[5].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                        f'{count}', ha='center', va='bottom', fontsize=12)
    else:
        axes[5].text(0.5, 0.5, 'âœ… æœªæ£€æµ‹åˆ°å¥‡ç‚¹\n(å·²æˆåŠŸä¿®å¤)', 
                    horizontalalignment='center', verticalalignment='center',
                    transform=axes[5].transAxes, fontsize=14)
        axes[5].set_title('å¥‡ç‚¹æ£€æµ‹ç»“æœ', fontsize=12)
        axes[5].set_xlim(0, 1)
        axes[5].set_ylim(0, 1)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print(f"\nâœ… ç»¼åˆä¿®å¤ç»“æœ:")
    print(f"  è®­ç»ƒæŸå¤±: {loss.item():.4f}")
    print(f"  æ£€æµ‹åˆ°å¥‡ç‚¹æ€»æ•°: {len(detected_singularities)}")
    print(f"  Type Iå¥‡ç‚¹æ•°é‡: {type_I_count}")
    print(f"  Type IIå¥‡ç‚¹æ•°é‡: {type_II_count}")
    
    if detected_singularities:
        print(f"  è¯¦ç»†å¥‡ç‚¹ä¿¡æ¯:")
        for i, sing in enumerate(detected_singularities):
            if sing['type'] == 'Type_I':
                print(f"    [{i+1}] Type I: å±‚={sing['layer']}, å€¼={sing['value']:.2f}, æ•°é‡={sing['count']}ä¸ª")
            else:
                print(f"    [{i+1}] Type II: {sing['layer_transition']}, "
                      f"è¿é€šåˆ†é‡å˜åŒ–={sing['n_components_change']}, "
                      f"å¯†åº¦å˜åŒ–={sing['density_change']:.3f}")
    
    # éªŒè¯ä¿®å¤æ•ˆæœ
    input_max = torch.max(torch.abs(inputs)).item()
    output_max = torch.max(torch.abs(outputs)).item()
    print(f"  ä¿®å¤å‰æœ€å¤§ç»å¯¹å€¼: {input_max:.2f}")
    print(f"  ä¿®å¤åæœ€å¤§ç»å¯¹å€¼: {output_max:.2f}")
    print(f"  ä¿®å¤æ•ˆæœ: {'âœ… æˆåŠŸ' if output_max <= 1.0 else 'âš ï¸ éƒ¨åˆ†ä¿®å¤'}")
    
    # æ‹“æ‰‘ç¨³å®šæ€§åˆ†æ
    input_topo = model.activations['input_topology']
    output_topo = model.activations['output_topology']
    topo_stable = abs(input_topo['n_components'] - output_topo['n_components']) <= 2
    print(f"  æ‹“æ‰‘ç¨³å®šæ€§: {'âœ… ç¨³å®š' if topo_stable else 'âš ï¸ å˜åŒ–è¾ƒå¤§'}")

# è¿è¡Œç»¼åˆå¯è§†åŒ–
if __name__ == "__main__":
    visualize_comprehensive_singularity_detection()
