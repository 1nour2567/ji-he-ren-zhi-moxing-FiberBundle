import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import math

class CognitiveFiberBundle(nn.Module):
    """è®¤çŸ¥çº¤ç»´ä¸›æ¨¡å‹ - é™ˆç±»è®¡ç®—çš„æœ€ç»ˆå®ç°"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.k_neighbors = k_neighbors
        
        # å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ç”¨äºé™ˆç±»è®¡ç®—
        self.activations = {}
        
        # è®¤çŸ¥çº¤ç»´ä¸›çš„è”ç»œå½¢å¼ï¼ˆconnection formï¼‰
        self.connection_form = nn.Parameter(torch.randn(d_model, d_model) * 0.1)
        
        # ç®€åŒ–çš„æ›²ç‡å½¢å¼è®¡ç®—å‚æ•°
        self.curvature_weight = nn.Parameter(torch.randn(d_model, d_model) * 0.01)
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            )
        ])
    
    def compute_connection_form(self, x):
        """è®¡ç®—è”ç»œå½¢å¼ A - ä¿®å¤ç‰ˆæœ¬"""
        batch_size, n, d = x.shape
        x_flat = x.view(batch_size * n, d)
        
        # æ‰©å±•connection_formåˆ°batchç»´åº¦
        connection = self.connection_form.unsqueeze(0).expand(batch_size * n, -1, -1)
        
        # ç®€åŒ–ï¼šä¸ºæ¯ä¸ªä½ç½®æ·»åŠ åŸºäºè¾“å…¥çš„å¯¹è§’æ‰°åŠ¨
        input_diag = torch.diag_embed(torch.mean(x_flat, dim=1, keepdim=True).expand(-1, d) * 0.01)
        connection = connection + input_diag
        
        return connection
    
    def compute_curvature_form(self, connection):
        """è®¡ç®—æ›²ç‡å½¢å¼ F = dA + Aâˆ§A (ç®€åŒ–ç‰ˆæœ¬)"""
        batch_size_n, d, d = connection.shape
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šF = A*A - A^T*A (ææ‹¬å·çš„å¯¹è§’å ä¼˜è¿‘ä¼¼)
        A_squared = torch.bmm(connection, connection)
        A_transposed = connection.transpose(-2, -1)
        A_transposed_squared = torch.bmm(A_transposed, connection)
        
        curvature = A_squared - A_transposed_squared
        return curvature
    
    def compute_chern_class(self, curvature):
        """è®¡ç®—é™ˆç±» - æ‹“æ‰‘ä¸å˜é‡"""
        batch_size, d, d = curvature.shape
        
        # ç¬¬ä¸€é™ˆç±»: c1 = tr(F) / (2Ï€i)
        trace_F = torch.diagonal(curvature, dim1=-2, dim2=-1).sum(dim=-1)  # tr(F)
        first_chern_class = trace_F / (2 * math.pi * 1j)  # å¤æ•°å½¢å¼
        
        # ç¬¬äºŒé™ˆç±»: c2 = (tr(FÂ²) - tr(F)Â²) / (8Ï€Â²)
        F_squared = torch.bmm(curvature, curvature)
        trace_F_squared = torch.diagonal(F_squared, dim1=-2, dim2=-1).sum(dim=-1)  # tr(FÂ²)
        
        second_chern_class = (trace_F_squared - trace_F**2) / (8 * math.pi**2)
        
        return {
            'first_chern_class': first_chern_class.real,  # å–å®éƒ¨
            'second_chern_class': second_chern_class,
            'trace_F': trace_F,
            'trace_F_squared': trace_F_squared
        }
    
    def compute_topological_invariants(self, x):
        """è®¡ç®—æ‹“æ‰‘ä¸å˜é‡ - ä¿®å¤ç‰ˆæœ¬"""
        batch_size, n, d = x.shape
        all_invariants = {'first_chern_class': [], 'second_chern_class': [], 
                         'trace_F': [], 'trace_F_squared': []}
        
        for i in range(n):
            x_slice = x[:, i:i+1, :]  # [batch_size, 1, d_model]
            
            connection = self.compute_connection_form(x_slice)
            curvature = self.compute_curvature_form(connection)
            chern_classes = self.compute_chern_class(curvature)
            
            all_invariants['first_chern_class'].append(chern_classes['first_chern_class'])
            all_invariants['second_chern_class'].append(chern_classes['second_chern_class'])
            all_invariants['trace_F'].append(chern_classes['trace_F'])
            all_invariants['trace_F_squared'].append(chern_classes['trace_F_squared'])
        
        # åˆå¹¶æ‰€æœ‰ä½ç½®çš„ç»“æœ
        result = {}
        for key in all_invariants:
            if all_invariants[key]:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                result[key] = torch.stack(all_invariants[key], dim=1)
            else:
                # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé€‚å½“å½¢çŠ¶çš„é›¶å¼ é‡
                result[key] = torch.zeros(batch_size, n, dtype=torch.float32, device=x.device)
        
        return result
    
    def forward(self, x):
        """x: [batch_size, vocab_size, d_model]"""
        batch_size, n, d = x.shape
        
        # ä¿å­˜è¾“å…¥ç”¨äºé™ˆç±»è®¡ç®—
        self.activations['input'] = x.clone()
        self.activations['input_topology'] = self.compute_topological_invariants(x)
        
        # åº”ç”¨å‡ ä½•å˜æ¢
        x_flat = x.view(-1, d)
        for i, layer in enumerate(self.geometric_layers):
            x_flat = layer(x_flat)
            layer_output = x_flat.view(batch_size, n, d)
            
            self.activations[f'layer{i+1}'] = layer_output.clone()
            self.activations[f'layer{i+1}_topology'] = self.compute_topological_invariants(layer_output)
        
        x = x_flat.view(batch_size, n, d)
        self.activations['output'] = x.clone()
        self.activations['output_topology'] = self.compute_topological_invariants(x)
        
        return x

class ChernClassAnalyzer:
    """é™ˆç±»åˆ†æå™¨"""
    def __init__(self, model):
        self.model = model
        self.chern_history = []
    
    def analyze_chern_classes(self):
        """åˆ†æå„å±‚çš„é™ˆç±»"""
        topology_keys = [k for k in self.model.activations.keys() if 'topology' in k]
        
        chern_analysis = {}
        
        for key in topology_keys:
            topo_info = self.model.activations[key]
            
            chern_analysis[key] = {
                'first_chern_mean': torch.mean(topo_info['first_chern_class']).item(),
                'first_chern_std': torch.std(topo_info['first_chern_class']).item(),
                'second_chern_mean': torch.mean(topo_info['second_chern_class']).item(),
                'second_chern_std': torch.std(topo_info['second_chern_class']).item(),
                'trace_F_mean': torch.mean(topo_info['trace_F']).item(),
                'trace_F_std': torch.std(topo_info['trace_F']).item()
            }
        
        return chern_analysis
    
    def detect_topological_phase_transitions(self):
        """æ£€æµ‹æ‹“æ‰‘ç›¸å˜ï¼ˆé™ˆç±»çš„çªå˜ï¼‰"""
        topology_keys = [k for k in self.model.activations.keys() if 'topology' in k]
        
        phase_transitions = []
        
        for i in range(len(topology_keys)-1):
            current_key = topology_keys[i]
            next_key = topology_keys[i+1]
            
            current_chern = self.model.activations[current_key]
            next_chern = self.model.activations[next_key]
            
            # è®¡ç®—é™ˆç±»å˜åŒ–
            first_chern_change = torch.mean(torch.abs(
                torch.mean(current_chern['first_chern_class'], dim=[0,1]) - 
                torch.mean(next_chern['first_chern_class'], dim=[0,1])
            )).item()
            
            second_chern_change = torch.mean(torch.abs(
                torch.mean(current_chern['second_chern_class'], dim=[0,1]) - 
                torch.mean(next_chern['second_chern_class'], dim=[0,1])
            )).item()
            
            # å¦‚æœé™ˆç±»å˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯æ‹“æ‰‘ç›¸å˜
            if first_chern_change > 0.1 or second_chern_change > 0.01:
                phase_transitions.append({
                    'transition': f"{current_key} -> {next_key}",
                    'first_chern_change': first_chern_change,
                    'second_chern_change': second_chern_change,
                    'detected': True
                })
        
        return phase_transitions

def test_topological_stability_under_anomalies():
    """æµ‹è¯•å¼‚å¸¸æ¡ä»¶ä¸‹çš„æ‹“æ‰‘ç¨³å®šæ€§"""
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 1
    vocab_size = 8
    d_model = 16  # å‡å°ç»´åº¦ä»¥ç®€åŒ–è®¡ç®—
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¼‚å¸¸æ¡ä»¶ä¸‹çš„æ‹“æ‰‘ç¨³å®šæ€§...")
    
    # æµ‹è¯•1: æ­£å¸¸æ¡ä»¶ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
    print("\nğŸ“‹ åŸºå‡†æµ‹è¯• - æ­£å¸¸è¾“å…¥:")
    model_normal = CognitiveFiberBundle(vocab_size, d_model=d_model)
    analyzer_normal = ChernClassAnalyzer(model_normal)
    
    inputs_normal = torch.randn(batch_size, vocab_size, d_model)
    outputs_normal = model_normal(inputs_normal)
    chern_analysis_normal = analyzer_normal.analyze_chern_classes()
    phase_transitions_normal = analyzer_normal.detect_topological_phase_transitions()
    
    print(f"  ç¬¬äºŒé™ˆç±»å˜åŒ–: {abs(chern_analysis_normal['input_topology']['second_chern_mean'] - chern_analysis_normal['output_topology']['second_chern_mean']):.6f}")
    print(f"  æ£€æµ‹åˆ°æ‹“æ‰‘ç›¸å˜: {len(phase_transitions_normal)}ä¸ª")
    
    # æµ‹è¯•2: é«˜å¼‚å¸¸å€¼è¾“å…¥
    print("\nâš ï¸ æµ‹è¯•2 - é«˜å¼‚å¸¸å€¼è¾“å…¥:")
    model_anomaly = CognitiveFiberBundle(vocab_size, d_model=d_model)
    analyzer_anomaly = ChernClassAnalyzer(model_anomaly)
    
    inputs_anomaly = torch.randn(batch_size, vocab_size, d_model)
    # æ·»åŠ å¼‚å¸¸å€¼
    inputs_anomaly[0, 0, 0] = 100.0    # æå¤§å¼‚å¸¸å€¼
    inputs_anomaly[0, 1, 5] = -100.0   # æå°å¼‚å¸¸å€¼
    inputs_anomaly[0, 2, 10] = 50.0    # ä¸­ç­‰å¼‚å¸¸å€¼
    
    print(f"  å¼‚å¸¸å€¼ä½ç½®å’Œå¤§å°: (0,0,0)={inputs_anomaly[0, 0, 0].item():.1f}, (0,1,5)={inputs_anomaly[0, 1, 5].item():.1f}, (0,2,10)={inputs_anomaly[0, 2, 10].item():.1f}")
    
    outputs_anomaly = model_anomaly(inputs_anomaly)
    chern_analysis_anomaly = analyzer_anomaly.analyze_chern_classes()
    phase_transitions_anomaly = analyzer_anomaly.detect_topological_phase_transitions()
    
    print(f"  ç¬¬äºŒé™ˆç±»å˜åŒ–: {abs(chern_analysis_anomaly['input_topology']['second_chern_mean'] - chern_analysis_anomaly['output_topology']['second_chern_mean']):.6f}")
    print(f"  æ£€æµ‹åˆ°æ‹“æ‰‘ç›¸å˜: {len(phase_transitions_anomaly)}ä¸ª")
    
    # æµ‹è¯•3: æ‹“æ‰‘ç»“æ„å¼‚å¸¸ï¼ˆé«˜è¿æ¥æ€§ç ´åï¼‰
    print("\nâš ï¸ æµ‹è¯•3 - æ‹“æ‰‘ç»“æ„å¼‚å¸¸è¾“å…¥:")
    model_topology = CognitiveFiberBundle(vocab_size, d_model=d_model)
    analyzer_topology = ChernClassAnalyzer(model_topology)
    
    inputs_topology = torch.randn(batch_size, vocab_size, d_model)
    # åˆ›å»ºæ‹“æ‰‘å¼‚å¸¸ï¼šæç«¯å€¼æ¨¡å¼
    inputs_topology[0, :4, :] = 20.0   # å‰4ä¸ªä½ç½®é«˜æ¿€æ´»
    inputs_topology[0, 4:, :] = -20.0  # å4ä¸ªä½ç½®è´Ÿæ¿€æ´»
    
    print(f"  æ‹“æ‰‘å¼‚å¸¸æ¨¡å¼: å‰4ä½ç½®={torch.mean(inputs_topology[0, :4, :]).item():.2f}, å4ä½ç½®={torch.mean(inputs_topology[0, 4:, :]).item():.2f}")
    
    outputs_topology = model_topology(inputs_topology)
    chern_analysis_topology = analyzer_topology.analyze_chern_classes()
    phase_transitions_topology = analyzer_topology.detect_topological_phase_transitions()
    
    print(f"  ç¬¬äºŒé™ˆç±»å˜åŒ–: {abs(chern_analysis_topology['input_topology']['second_chern_mean'] - chern_analysis_topology['output_topology']['second_chern_mean']):.6f}")
    print(f"  æ£€æµ‹åˆ°æ‹“æ‰‘ç›¸å˜: {len(phase_transitions_topology)}ä¸ª")
    
    # æµ‹è¯•4: çº¦æŸè¿åå¼‚å¸¸
    print("\nâš ï¸ æµ‹è¯•4 - çº¦æŸè¿åå¼‚å¸¸è¾“å…¥:")
    model_constraint = CognitiveFiberBundle(vocab_size, d_model=d_model)
    analyzer_constraint = ChernClassAnalyzer(model_constraint)
    
    inputs_constraint = torch.randn(batch_size, vocab_size, d_model)
    # åˆ›å»ºçº¦æŸè¿åï¼šç ´åä¸€è‡´æ€§
    inputs_constraint[0, 0, :] = torch.ones(d_model) * 10  # ä¸€è‡´æ€§ç ´å
    inputs_constraint[0, 1, :] = torch.ones(d_model) * -10 # ä¸€è‡´æ€§ç ´å
    inputs_constraint[0, 2, :] = torch.ones(d_model) * 5   # ä¼ é€’æ€§ç ´å
    
    print(f"  çº¦æŸå¼‚å¸¸æ¨¡å¼: ä½ç½®0={torch.mean(inputs_constraint[0, 0, :]).item():.2f}, ä½ç½®1={torch.mean(inputs_constraint[0, 1, :]).item():.2f}, ä½ç½®2={torch.mean(inputs_constraint[0, 2, :]).item():.2f}")
    
    outputs_constraint = model_constraint(inputs_constraint)
    chern_analysis_constraint = analyzer_constraint.analyze_chern_classes()
    phase_transitions_constraint = analyzer_constraint.detect_topological_phase_transitions()
    
    print(f"  ç¬¬äºŒé™ˆç±»å˜åŒ–: {abs(chern_analysis_constraint['input_topology']['second_chern_mean'] - chern_analysis_constraint['output_topology']['second_chern_mean']):.6f}")
    print(f"  æ£€æµ‹åˆ°æ‹“æ‰‘ç›¸å˜: {len(phase_transitions_constraint)}ä¸ª")
    
    # ç»“æœæ±‡æ€»
    print(f"\nğŸ“Š å¼‚å¸¸æ¡ä»¶ä¸‹çš„æ‹“æ‰‘ç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
    print(f"  æ­£å¸¸è¾“å…¥: ç¬¬äºŒé™ˆç±»å˜åŒ–={abs(chern_analysis_normal['input_topology']['second_chern_mean'] - chern_analysis_normal['output_topology']['second_chern_mean']):.6f}, æ‹“æ‰‘ç›¸å˜={len(phase_transitions_normal)}ä¸ª")
    print(f"  é«˜å¼‚å¸¸å€¼: ç¬¬äºŒé™ˆç±»å˜åŒ–={abs(chern_analysis_anomaly['input_topology']['second_chern_mean'] - chern_analysis_anomaly['output_topology']['second_chern_mean']):.6f}, æ‹“æ‰‘ç›¸å˜={len(phase_transitions_anomaly)}ä¸ª")
    print(f"  æ‹“æ‰‘å¼‚å¸¸: ç¬¬äºŒé™ˆç±»å˜åŒ–={abs(chern_analysis_topology['input_topology']['second_chern_mean'] - chern_analysis_topology['output_topology']['second_chern_mean']):.6f}, æ‹“æ‰‘ç›¸å˜={len(phase_transitions_topology)}ä¸ª")
    print(f"  çº¦æŸå¼‚å¸¸: ç¬¬äºŒé™ˆç±»å˜åŒ–={abs(chern_analysis_constraint['input_topology']['second_chern_mean'] - chern_analysis_constraint['output_topology']['second_chern_mean']):.6f}, æ‹“æ‰‘ç›¸å˜={len(phase_transitions_constraint)}ä¸ª")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()
    
    # ç¬¬äºŒé™ˆç±»å˜åŒ–å¯¹æ¯”
    test_names = ['æ­£å¸¸è¾“å…¥', 'é«˜å¼‚å¸¸å€¼', 'æ‹“æ‰‘å¼‚å¸¸', 'çº¦æŸå¼‚å¸¸']
    chern_changes = [
        abs(chern_analysis_normal['input_topology']['second_chern_mean'] - chern_analysis_normal['output_topology']['second_chern_mean']),
        abs(chern_analysis_anomaly['input_topology']['second_chern_mean'] - chern_analysis_anomaly['output_topology']['second_chern_mean']),
        abs(chern_analysis_topology['input_topology']['second_chern_mean'] - chern_analysis_topology['output_topology']['second_chern_mean']),
        abs(chern_analysis_constraint['input_topology']['second_chern_mean'] - chern_analysis_constraint['output_topology']['second_chern_mean'])
    ]
    
    axes[0].bar(test_names, chern_changes, color=['green', 'orange', 'red', 'purple'], alpha=0.7)
    axes[0].set_title('ç¬¬äºŒé™ˆç±»å˜åŒ–å¯¹æ¯”', fontsize=12)
    axes[0].set_ylabel('ç¬¬äºŒé™ˆç±»å˜åŒ–é‡')
    axes[0].tick_params(axis='x', rotation=45)
    
    # æ‹“æ‰‘ç›¸å˜æ•°é‡å¯¹æ¯”
    phase_transitions_counts = [
        len(phase_transitions_normal),
        len(phase_transitions_anomaly),
        len(phase_transitions_topology),
        len(phase_transitions_constraint)
    ]
    
    axes[1].bar(test_names, phase_transitions_counts, color=['green', 'orange', 'red', 'purple'], alpha=0.7)
    axes[1].set_title('æ£€æµ‹åˆ°çš„æ‹“æ‰‘ç›¸å˜æ•°é‡', fontsize=12)
    axes[1].set_ylabel('æ‹“æ‰‘ç›¸å˜æ•°é‡')
    axes[1].tick_params(axis='x', rotation=45)
    
    # å„æµ‹è¯•çš„é™ˆç±»åˆ†å¸ƒ
    all_chern_data = [
        model_normal.activations['input_topology']['second_chern_class'].detach().numpy().flatten(),
        model_anomaly.activations['input_topology']['second_chern_class'].detach().numpy().flatten(),
        model_topology.activations['input_topology']['second_chern_class'].detach().numpy().flatten(),
        model_constraint.activations['input_topology']['second_chern_class'].detach().numpy().flatten()
    ]
    
    colors = ['green', 'orange', 'red', 'purple']
    for i, (data, name, color) in enumerate(zip(all_chern_data, test_names, colors)):
        axes[2].hist(data, bins=15, alpha=0.5, label=name, color=color)
    axes[2].set_title('å„æµ‹è¯•è¾“å…¥çš„ç¬¬äºŒé™ˆç±»åˆ†å¸ƒ', fontsize=12)
    axes[2].set_xlabel('ç¬¬äºŒé™ˆç±»å€¼')
    axes[2].set_ylabel('é¢‘æ¬¡')
    axes[2].legend()
    
    # è¾“å‡ºå±‚é™ˆç±»å¯¹æ¯”
    output_chern_data = [
        model_normal.activations['output_topology']['second_chern_class'].detach().numpy().flatten(),
        model_anomaly.activations['output_topology']['second_chern_class'].detach().numpy().flatten(),
        model_topology.activations['output_topology']['second_chern_class'].detach().numpy().flatten(),
        model_constraint.activations['output_topology']['second_chern_class'].detach().numpy().flatten()
    ]
    
    for i, (data, name, color) in enumerate(zip(output_chern_data, test_names, colors)):
        axes[3].hist(data, bins=15, alpha=0.5, label=name, color=color)
    axes[3].set_title('å„æµ‹è¯•è¾“å‡ºçš„ç¬¬äºŒé™ˆç±»åˆ†å¸ƒ', fontsize=12)
    axes[3].set_xlabel('ç¬¬äºŒé™ˆç±»å€¼')
    axes[3].set_ylabel('é¢‘æ¬¡')
    axes[3].legend()
    
    plt.tight_layout()
    plt.show()
    
    # ç¨³å®šæ€§è¯„ä¼°
    print(f"\nğŸ¯ æ‹“æ‰‘ç¨³å®šæ€§è¯„ä¼°:")
    stable_tests = 0
    for i, (change, trans) in enumerate(zip(chern_changes, phase_transitions_counts)):
        is_stable = change < 0.01 and trans == 0
        status = "âœ… ç¨³å®š" if is_stable else "âš ï¸ ä¸ç¨³å®š"
        print(f"  {test_names[i]}: {status} (å˜åŒ–:{change:.6f}, ç›¸å˜:{trans}ä¸ª)")
        if is_stable:
            stable_tests += 1
    
    overall_stability = f"âœ… é«˜ç¨³å®šæ€§ ({stable_tests}/4ç¨³å®š)" if stable_tests >= 3 else f"âš ï¸ éƒ¨åˆ†ç¨³å®šæ€§ ({stable_tests}/4ç¨³å®š)"
    print(f"\nğŸ“ˆ æ€»ä½“ç¨³å®šæ€§: {overall_stability}")
    
    return {
        'normal': chern_analysis_normal,
        'anomaly': chern_analysis_anomaly,
        'topology': chern_analysis_topology,
        'constraint': chern_analysis_constraint,
        'phase_transitions': {
            'normal': len(phase_transitions_normal),
            'anomaly': len(phase_transitions_anomaly),
            'topology': len(phase_transitions_topology),
            'constraint': len(phase_transitions_constraint)
        }
    }

# è¿è¡Œå¼‚å¸¸æ¡ä»¶æµ‹è¯•
if __name__ == "__main__":
    results = test_topological_stability_under_anomalies()
