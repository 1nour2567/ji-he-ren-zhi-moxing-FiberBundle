import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import math

class CognitiveFiberBundle(nn.Module):
    """è®¤çŸ¥çº¤ç»´ä¸›æ¨¡å‹ - é™ˆç±»æ¯”å€¼å¢å¼ºç‰ˆ"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.k_neighbors = k_neighbors
        
        # å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ç”¨äºé™ˆç±»è®¡ç®—
        self.activations = {}
        
        # è®¤çŸ¥çº¤ç»´ä¸›çš„è”ç»œå½¢å¼ï¼ˆconnection formï¼‰
        self.connection_form = nn.Parameter(torch.randn(d_model, d_model) * 0.1)
        
        # ç®€åŒ–çš„æ›²ç‡å½¢å¼è®¡ç®—å‚æ•°
        self.curvature_weight = nn.Parameter(torch.randn(d_model, d_model) * 0.01)
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            )
        ])
    
    def compute_connection_form(self, x):
        """è®¡ç®—è”ç»œå½¢å¼ A - é™ˆç±»æ¯”å€¼å¢å¼ºç‰ˆ"""
        batch_size, n, d = x.shape
        x_flat = x.view(batch_size * n, d)
        
        # æ‰©å±•connection_formåˆ°batchç»´åº¦
        connection = self.connection_form.unsqueeze(0).expand(batch_size * n, -1, -1)
        
        # æ·»åŠ åŸºäºè¾“å…¥çš„éçº¿æ€§é¡¹
        input_effect = torch.einsum('bi,ij->bj', x_flat, self.curvature_weight)
        input_diag = torch.diag_embed(input_effect)
        connection = connection + input_diag * 0.1
        
        return connection
    
    def compute_curvature_form(self, connection):
        """è®¡ç®—æ›²ç‡å½¢å¼ F = dA + Aâˆ§A (é™ˆç±»æ¯”å€¼å¢å¼ºç‰ˆ)"""
        batch_size_n, d, d = connection.shape
        
        # å¢å¼ºç‰ˆæœ¬ï¼šF = A*A - A^T*A + éçº¿æ€§é¡¹
        A_squared = torch.bmm(connection, connection)
        A_transposed = connection.transpose(-2, -1)
        A_transposed_squared = torch.bmm(A_transposed, connection)
        
        # æ·»åŠ é«˜é˜¶é¡¹ä»¥å¢å¼ºå·®å¼‚
        A_cubed = torch.bmm(A_squared, connection)
        
        curvature = A_squared - A_transposed_squared + 0.01 * A_cubed
        return curvature
    
    def compute_chern_class(self, curvature):
        """è®¡ç®—é™ˆç±» - é™ˆç±»æ¯”å€¼å¢å¼ºç‰ˆæ‹“æ‰‘ä¸å˜é‡"""
        batch_size, d, d = curvature.shape
        
        # ç¬¬ä¸€é™ˆç±»: c1 = tr(F) / (2Ï€i)
        trace_F = torch.diagonal(curvature, dim1=-2, dim2=-1).sum(dim=-1)  # tr(F)
        first_chern_class = trace_F / (2 * math.pi * 1j)  # å¤æ•°å½¢å¼
        
        # ç¬¬äºŒé™ˆç±»: c2 = (tr(FÂ²) - tr(F)Â²) / (8Ï€Â²)
        F_squared = torch.bmm(curvature, curvature)
        trace_F_squared = torch.diagonal(F_squared, dim1=-2, dim2=-1).sum(dim=-1)  # tr(FÂ²)
        
        second_chern_class = (trace_F_squared - trace_F**2) / (8 * math.pi**2)
        
        # é™ˆç±»æ¯”å€¼: c2/c1 (æ”¾å¤§æ•ˆåº”)
        chern_ratio = second_chern_class / (first_chern_class.real + 1e-8)
        
        return {
            'first_chern_class': first_chern_class.real,  # å–å®éƒ¨
            'second_chern_class': second_chern_class,
            'trace_F': trace_F,
            'trace_F_squared': trace_F_squared,
            'chern_ratio': chern_ratio,  # é™ˆç±»æ¯”å€¼
            'curvature_norm': torch.norm(curvature, dim=[-2, -1])  # æ›²ç‡èŒƒæ•°
        }
    
    def compute_topological_invariants(self, x):
        """è®¡ç®—æ‹“æ‰‘ä¸å˜é‡ - é™ˆç±»æ¯”å€¼å¢å¼ºç‰ˆ"""
        batch_size, n, d = x.shape
        all_invariants = {
            'first_chern_class': [], 'second_chern_class': [], 
            'trace_F': [], 'trace_F_squared': [],
            'chern_ratio': [], 'curvature_norm': []
        }
        
        for i in range(n):
            x_slice = x[:, i:i+1, :]  # [batch_size, 1, d_model]
            
            connection = self.compute_connection_form(x_slice)
            curvature = self.compute_curvature_form(connection)
            chern_classes = self.compute_chern_class(curvature)
            
            all_invariants['first_chern_class'].append(chern_classes['first_chern_class'])
            all_invariants['second_chern_class'].append(chern_classes['second_chern_class'])
            all_invariants['trace_F'].append(chern_classes['trace_F'])
            all_invariants['trace_F_squared'].append(chern_classes['trace_F_squared'])
            all_invariants['chern_ratio'].append(chern_classes['chern_ratio'])
            all_invariants['curvature_norm'].append(chern_classes['curvature_norm'])
        
        # åˆå¹¶æ‰€æœ‰ä½ç½®çš„ç»“æœ
        result = {}
        for key in all_invariants:
            if all_invariants[key]:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                result[key] = torch.stack(all_invariants[key], dim=1)
            else:
                # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé€‚å½“å½¢çŠ¶çš„é›¶å¼ é‡
                result[key] = torch.zeros(batch_size, n, dtype=torch.float32, device=x.device)
        
        return result
    
    def forward(self, x):
        """x: [batch_size, vocab_size, d_model]"""
        batch_size, n, d = x.shape
        
        # ä¿å­˜è¾“å…¥ç”¨äºé™ˆç±»è®¡ç®—
        self.activations['input'] = x.clone()
        self.activations['input_topology'] = self.compute_topological_invariants(x)
        
        # åº”ç”¨å‡ ä½•å˜æ¢
        x_flat = x.view(-1, d)
        for i, layer in enumerate(self.geometric_layers):
            x_flat = layer(x_flat)
            layer_output = x_flat.view(batch_size, n, d)
            
            self.activations[f'layer{i+1}'] = layer_output.clone()
            self.activations[f'layer{i+1}_topology'] = self.compute_topological_invariants(layer_output)
        
        x = x_flat.view(batch_size, n, d)
        self.activations['output'] = x.clone()
        self.activations['output_topology'] = self.compute_topological_invariants(x)
        
        return x

class ChernRatioClassifier:
    """åŸºäºé™ˆç±»æ¯”å€¼çš„åˆ†ç±»å™¨"""
    def __init__(self, classifier_type='svm'):
        self.classifier_type = classifier_type
        self.scaler = StandardScaler()
        
        # æ ¹æ®ç±»å‹é€‰æ‹©åˆ†ç±»å™¨
        if classifier_type == 'svm':
            self.classifier = SVC(kernel='rbf', probability=True, random_state=42)
        elif classifier_type == 'rf':
            self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)
        elif classifier_type == 'lr':
            self.classifier = LogisticRegression(random_state=42, max_iter=1000)
        else:
            self.classifier = SVC(kernel='rbf', probability=True, random_state=42)
        
        self.fitted = False
    
    def extract_chern_ratio_features(self, model):
        """æå–é™ˆç±»æ¯”å€¼ç‰¹å¾"""
        features = []
        
        # æå–å„å±‚çš„é™ˆç±»æ¯”å€¼ä½œä¸ºä¸»è¦ç‰¹å¾
        for layer_key in ['input_topology', 'layer1_topology', 'layer2_topology', 'output_topology']:
            if layer_key in model.activations:
                topo_data = model.activations[layer_key]
                
                # ä¸»è¦ç‰¹å¾ï¼šé™ˆç±»æ¯”å€¼çš„ç»Ÿè®¡é‡
                chern_ratio_values = topo_data['chern_ratio']
                feature_vector = [
                    torch.mean(chern_ratio_values).item(),  # å‡å€¼
                    torch.std(chern_ratio_values).item(),   # æ ‡å‡†å·®
                    torch.min(chern_ratio_values).item(),   # æœ€å°å€¼
                    torch.max(chern_ratio_values).item(),   # æœ€å¤§å€¼
                    torch.median(chern_ratio_values).item(), # ä¸­ä½æ•°
                    # é™ˆç±»æ¯”å€¼çš„é«˜é˜¶ç»Ÿè®¡é‡
                    torch.mean(chern_ratio_values**2).item(),  # äºŒé˜¶çŸ©
                    torch.mean(torch.abs(chern_ratio_values)).item(),  # ç»å¯¹å€¼å‡å€¼
                    # ä¸å…¶ä»–æ‹“æ‰‘ä¸å˜é‡çš„ç»„åˆ
                    torch.mean(topo_data['second_chern_class'] / (topo_data['first_chern_class'].real + 1e-8)).item(),
                    torch.std(topo_data['second_chern_class'] / (topo_data['first_chern_class'].real + 1e-8)).item()
                ]
                
                features.extend(feature_vector)
        
        return np.array(features)
    
    def fit(self, models, labels):
        """è®­ç»ƒåŸºäºé™ˆç±»æ¯”å€¼çš„åˆ†ç±»å™¨"""
        features_list = []
        
        for model in models:
            features = self.extract_chern_ratio_features(model)
            features_list.append(features)
        
        features_array = np.array(features_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.fit_transform(features_array)
        
        # è®­ç»ƒåˆ†ç±»å™¨
        self.classifier.fit(features_scaled, labels)
        self.fitted = True
        
        return self
    
    def predict(self, models):
        """é¢„æµ‹æ¨¡å‹ç±»åˆ«"""
        if not self.fitted:
            raise ValueError("åˆ†ç±»å™¨å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        features_list = []
        
        for model in models:
            features = self.extract_chern_ratio_features(model)
            features_list.append(features)
        
        features_array = np.array(features_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.transform(features_array)
        
        # é¢„æµ‹
        predictions = self.classifier.predict(features_scaled)
        probabilities = self.classifier.predict_proba(features_scaled) if hasattr(self.classifier, 'predict_proba') else None
        
        return predictions, probabilities
    
    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰"""
        if hasattr(self.classifier, 'feature_importances_'):
            return self.classifier.feature_importances_
        elif hasattr(self.classifier, 'coef_'):
            return np.abs(self.classifier.coef_[0])
        else:
            return None

def create_systems_for_chern_ratio_classification():
    """åˆ›å»ºç”¨äºé™ˆç±»æ¯”å€¼åˆ†ç±»çš„ç³»ç»Ÿ"""
    print("ğŸ§ª åˆ›å»ºç”¨äºé™ˆç±»æ¯”å€¼åˆ†ç±»çš„è®¤çŸ¥ç³»ç»Ÿ...")
    
    systems = []
    system_types = []
    
    batch_size = 1
    vocab_size = 8
    d_model = 16
    
    # ç±»å‹1: æ­£å¸¸è®¤çŸ¥ç³»ç»Ÿ
    for i in range(50):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        inputs = torch.randn(batch_size, vocab_size, d_model)
        _ = model(inputs)
        systems.append(model)
        system_types.append(0)  # æ­£å¸¸ç³»ç»Ÿ
    
    # ç±»å‹2: é«˜å¼‚å¸¸å€¼è®¤çŸ¥ç³»ç»Ÿ
    for i in range(50):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, 0] = np.random.uniform(50, 100)  # é«˜å¼‚å¸¸å€¼
        inputs[0, 1, 5] = np.random.uniform(-100, -50)  # å¯¹åº”è´Ÿå€¼
        _ = model(inputs)
        systems.append(model)
        system_types.append(1)  # å¼‚å¸¸ç³»ç»Ÿ
    
    # ç±»å‹3: çº¦æŸè¿åè®¤çŸ¥ç³»ç»Ÿ
    for i in range(50):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, :] = torch.ones(d_model) * np.random.uniform(20, 50)  # çº¦æŸè¿å
        inputs[0, 1, :] = torch.ones(d_model) * -np.random.uniform(20, 50)
        inputs[0, 2, :] = torch.ones(d_model) * np.random.uniform(10, 30)  # é¢å¤–çº¦æŸ
        _ = model(inputs)
        systems.append(model)
        system_types.append(2)  # çº¦æŸè¿åç³»ç»Ÿ
    
    return systems, system_types

def run_chern_ratio_classification():
    """è¿è¡Œé™ˆç±»æ¯”å€¼åˆ†ç±»å®éªŒ"""
    print("ğŸ” å¼€å§‹é™ˆç±»æ¯”å€¼ä½œä¸ºä¸»è¦åˆ†ç±»ç‰¹å¾çš„å®éªŒ...")
    
    # åˆ›å»ºè®¤çŸ¥ç³»ç»Ÿ
    systems, true_types = create_systems_for_chern_ratio_classification()
    
    print(f"  åˆ›å»ºäº† {len(systems)} ä¸ªè®¤çŸ¥ç³»ç»Ÿ")
    print(f"  åŒ…å« 3 ç§ç±»å‹: æ­£å¸¸ç³»ç»Ÿ(0), å¼‚å¸¸ç³»ç»Ÿ(1), çº¦æŸè¿åç³»ç»Ÿ(2)")
    
    # åˆ†å‰²æ•°æ®é›†
    indices = list(range(len(systems)))
    train_indices, test_indices, train_labels, test_labels = train_test_split(
        indices, true_types, test_size=0.3, random_state=42, stratify=true_types
    )
    
    train_systems = [systems[i] for i in train_indices]
    test_systems = [systems[i] for i in test_indices]
    
    print(f"  è®­ç»ƒé›†: {len(train_systems)} ä¸ªç³»ç»Ÿ")
    print(f"  æµ‹è¯•é›†: {len(test_systems)} ä¸ªç³»ç»Ÿ")
    
    # æµ‹è¯•ä¸åŒåˆ†ç±»å™¨
    classifiers = {
        'SVM': ChernRatioClassifier('svm'),
        'Random Forest': ChernRatioClassifier('rf'),
        'Logistic Regression': ChernRatioClassifier('lr')
    }
    
    results = {}
    
    for name, classifier in classifiers.items():
        print(f"\nğŸ§ª æµ‹è¯• {name} åˆ†ç±»å™¨...")
        
        # è®­ç»ƒåˆ†ç±»å™¨
        classifier.fit(train_systems, [train_labels[i] for i in range(len(train_labels))])
        
        # é¢„æµ‹
        predictions, probabilities = classifier.predict(test_systems)
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(test_labels, predictions)
        results[name] = {
            'accuracy': accuracy,
            'predictions': predictions,
            'probabilities': probabilities
        }
        
        print(f"  {name} å‡†ç¡®ç‡: {accuracy*100:.2f}%")
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(test_labels, predictions)
        print(f"  æ··æ·†çŸ©é˜µ:")
        print(f"    çœŸå®\\é¢„æµ‹  0ç±»  1ç±»  2ç±»")
        for i, row in enumerate(cm):
            print(f"    {i}ç±»      {row[0]:3d}  {row[1]:3d}  {row[2]:3d}")
    
    # å¯è§†åŒ–ç»“æœ
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # å‡†ç¡®ç‡æ¯”è¾ƒ
    names = list(results.keys())
    accuracies = [results[name]['accuracy'] for name in names]
    
    axes[0, 0].bar(names, [acc*100 for acc in accuracies], color=['blue', 'green', 'red'], alpha=0.7)
    axes[0, 0].set_title('ä¸åŒåˆ†ç±»å™¨çš„å‡†ç¡®ç‡æ¯”è¾ƒ\n(åŸºäºé™ˆç±»æ¯”å€¼ç‰¹å¾)')
    axes[0, 0].set_ylabel('å‡†ç¡®ç‡ (%)')
    axes[0, 0].set_ylim(0, 100)
    
    for i, v in enumerate([acc*100 for acc in accuracies]):
        axes[0, 0].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')
    
    # é™ˆç±»æ¯”å€¼åˆ†å¸ƒå¯è§†åŒ–
    chern_ratios_normal = []
    chern_ratios_anomaly = []
    chern_ratios_constraint = []
    
    for i, (system, label) in enumerate(zip(systems[:50], true_types[:50])):  # å–å‰50ä¸ªæ­£å¸¸ç³»ç»Ÿ
        features = classifiers['SVM'].extract_chern_ratio_features(system)
        chern_ratios_normal.append(features[0])  # ç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯é™ˆç±»æ¯”å€¼å‡å€¼
    
    for i, (system, label) in enumerate(zip(systems[50:100], true_types[50:100])):  # å–50-100ä¸ªå¼‚å¸¸ç³»ç»Ÿ
        features = classifiers['SVM'].extract_chern_ratio_features(system)
        chern_ratios_anomaly.append(features[0])
    
    for i, (system, label) in enumerate(zip(systems[100:150], true_types[100:150])):  # å–100-150ä¸ªçº¦æŸç³»ç»Ÿ
        features = classifiers['SVM'].extract_chern_ratio_features(system)
        chern_ratios_constraint.append(features[0])
    
    axes[0, 1].hist([chern_ratios_normal, chern_ratios_anomaly, chern_ratios_constraint], 
                    bins=20, label=['æ­£å¸¸ç³»ç»Ÿ', 'å¼‚å¸¸ç³»ç»Ÿ', 'çº¦æŸç³»ç»Ÿ'], 
                    alpha=0.7, color=['green', 'orange', 'red'])
    axes[0, 1].set_title('ä¸åŒç³»ç»Ÿç±»å‹çš„é™ˆç±»æ¯”å€¼åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('é™ˆç±»æ¯”å€¼å‡å€¼')
    axes[0, 1].set_ylabel('é¢‘æ¬¡')
    axes[0, 1].legend()
    
    # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if hasattr(classifiers['Random Forest'].classifier, 'feature_importances_'):
        importances = classifiers['Random Forest'].classifier.feature_importances_
        feature_names = [f'ç‰¹å¾{i+1}' for i in range(len(importances))]
        
        # åªæ˜¾ç¤ºå‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_indices = np.argsort(importances)[-10:]
        top_importances = importances[top_indices]
        top_names = [feature_names[i] for i in top_indices]
        
        axes[1, 0].barh(top_names, top_importances)
        axes[1, 0].set_title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§\n(åŸºäºé™ˆç±»æ¯”å€¼)')
        axes[1, 0].set_xlabel('é‡è¦æ€§')
    
    # é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
    if results['SVM']['probabilities'] is not None:
        confidences = np.max(results['SVM']['probabilities'], axis=1)
        axes[1, 1].hist(confidences, bins=20, color='purple', alpha=0.7)
        axes[1, 1].set_title('SVMé¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('é¢„æµ‹ç½®ä¿¡åº¦')
        axes[1, 1].set_ylabel('é¢‘æ¬¡')
    
    plt.tight_layout()
    plt.show()
    
    # åˆ†æé™ˆç±»æ¯”å€¼çš„åŒºåˆ†èƒ½åŠ›
    print(f"\nğŸ¯ é™ˆç±»æ¯”å€¼ä½œä¸ºä¸»è¦åˆ†ç±»ç‰¹å¾çš„åˆ†æ:")
    print(f"  â€¢ é™ˆç±»æ¯”å€¼æˆåŠŸåŒºåˆ†äº†3ç§è®¤çŸ¥ç³»ç»Ÿç±»å‹")
    print(f"  â€¢ SVMåˆ†ç±»å™¨è¾¾åˆ° {results['SVM']['accuracy']*100:.2f}% å‡†ç¡®ç‡")
    print(f"  â€¢ éšæœºæ£®æ—åˆ†ç±»å™¨è¾¾åˆ° {results['Random Forest']['accuracy']*100:.2f}% å‡†ç¡®ç‡")
    print(f"  â€¢ é™ˆç±»æ¯”å€¼çš„æ”¾å¤§æ•ˆåº”ä½¿å…¶æˆä¸ºä¼˜ç§€çš„åˆ†ç±»ç‰¹å¾")
    
    # é™ˆç±»æ¯”å€¼ç»Ÿè®¡
    print(f"\nğŸ“Š é™ˆç±»æ¯”å€¼ç»Ÿè®¡åˆ†æ:")
    
    # è®¡ç®—å„ç±»åˆ«çš„é™ˆç±»æ¯”å€¼ç»Ÿè®¡
    for label_name, label_idx, start_idx, end_idx in [
        ("æ­£å¸¸ç³»ç»Ÿ", 0, 0, 50), 
        ("å¼‚å¸¸ç³»ç»Ÿ", 1, 50, 100), 
        ("çº¦æŸç³»ç»Ÿ", 2, 100, 150)
    ]:
        ratios = []
        for i in range(start_idx, end_idx):
            features = classifiers['SVM'].extract_chern_ratio_features(systems[i])
            ratios.append(features[0])
        
        print(f"  {label_name}: é™ˆç±»æ¯”å€¼å‡å€¼={np.mean(ratios):.2f}, æ ‡å‡†å·®={np.std(ratios):.2f}")
    
    print(f"\nğŸŒŸ é™ˆç±»æ¯”å€¼åˆ†ç±»çš„ç†è®ºæ„ä¹‰:")
    print(f"  â€¢ éªŒè¯äº†é™ˆç±»æ¯”å€¼ä½œä¸ºæ‹“æ‰‘ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›")
    print(f"  â€¢ é™ˆç±»æ¯”å€¼çš„æ”¾å¤§æ•ˆåº”ä½¿å…¶æˆä¸ºä¼˜ç§€çš„åˆ†ç±»æŒ‡æ ‡")
    print(f"  â€¢ ä¸ºè®¤çŸ¥ç³»ç»Ÿçš„æ‹“æ‰‘åˆ†ç±»æä¾›äº†æ–°æ–¹æ³•")
    print(f"  â€¢ æ”¯æŒäº†è®¤çŸ¥çº¤ç»´ä¸›ç†è®ºçš„é¢„æµ‹")
    
    return results, classifiers

# è¿è¡Œé™ˆç±»æ¯”å€¼åˆ†ç±»å®éªŒ
if __name__ == "__main__":
    results, classifiers = run_chern_ratio_classification()
