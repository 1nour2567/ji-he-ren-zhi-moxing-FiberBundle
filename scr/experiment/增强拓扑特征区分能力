import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import math

class CognitiveFiberBundle(nn.Module):
    """è®¤çŸ¥çº¤ç»´ä¸›æ¨¡å‹ - å¢å¼ºç‰ˆé™ˆç±»è®¡ç®—"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.k_neighbors = k_neighbors
        
        # å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ç”¨äºé™ˆç±»è®¡ç®—
        self.activations = {}
        
        # è®¤çŸ¥çº¤ç»´ä¸›çš„è”ç»œå½¢å¼ï¼ˆconnection formï¼‰
        self.connection_form = nn.Parameter(torch.randn(d_model, d_model) * 0.1)
        
        # ç®€åŒ–çš„æ›²ç‡å½¢å¼è®¡ç®—å‚æ•°
        self.curvature_weight = nn.Parameter(torch.randn(d_model, d_model) * 0.01)
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            )
        ])
    
    def compute_connection_form(self, x):
        """è®¡ç®—è”ç»œå½¢å¼ A - å¢å¼ºç‰ˆæœ¬"""
        batch_size, n, d = x.shape
        x_flat = x.view(batch_size * n, d)
        
        # æ‰©å±•connection_formåˆ°batchç»´åº¦
        connection = self.connection_form.unsqueeze(0).expand(batch_size * n, -1, -1)
        
        # æ·»åŠ åŸºäºè¾“å…¥çš„éçº¿æ€§é¡¹ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
        input_effect = torch.einsum('bi,ij->bj', x_flat, self.curvature_weight)
        input_diag = torch.diag_embed(input_effect)
        connection = connection + input_diag * 0.1
        
        return connection
    
    def compute_curvature_form(self, connection):
        """è®¡ç®—æ›²ç‡å½¢å¼ F = dA + Aâˆ§A (å¢å¼ºç‰ˆæœ¬)"""
        batch_size_n, d, d = connection.shape
        
        # å¢å¼ºç‰ˆæœ¬ï¼šF = A*A - A^T*A + éçº¿æ€§é¡¹
        A_squared = torch.bmm(connection, connection)
        A_transposed = connection.transpose(-2, -1)
        A_transposed_squared = torch.bmm(A_transposed, connection)
        
        # æ·»åŠ é«˜é˜¶é¡¹ä»¥å¢å¼ºå·®å¼‚
        A_cubed = torch.bmm(A_squared, connection)
        
        curvature = A_squared - A_transposed_squared + 0.01 * A_cubed
        return curvature
    
    def compute_chern_class(self, curvature):
        """è®¡ç®—é™ˆç±» - å¢å¼ºç‰ˆæ‹“æ‰‘ä¸å˜é‡"""
        batch_size, d, d = curvature.shape
        
        # ç¬¬ä¸€é™ˆç±»: c1 = tr(F) / (2Ï€i)
        trace_F = torch.diagonal(curvature, dim1=-2, dim2=-1).sum(dim=-1)  # tr(F)
        first_chern_class = trace_F / (2 * math.pi * 1j)  # å¤æ•°å½¢å¼
        
        # ç¬¬äºŒé™ˆç±»: c2 = (tr(FÂ²) - tr(F)Â²) / (8Ï€Â²)
        F_squared = torch.bmm(curvature, curvature)
        trace_F_squared = torch.diagonal(F_squared, dim1=-2, dim2=-1).sum(dim=-1)  # tr(FÂ²)
        
        second_chern_class = (trace_F_squared - trace_F**2) / (8 * math.pi**2)
        
        return {
            'first_chern_class': first_chern_class.real,  # å–å®éƒ¨
            'second_chern_class': second_chern_class,
            'trace_F': trace_F,
            'trace_F_squared': trace_F_squared,
            'chern_ratio': second_chern_class / (first_chern_class.real + 1e-8),  # é™ˆç±»æ¯”å€¼
            'curvature_norm': torch.norm(curvature, dim=[-2, -1])  # æ›²ç‡èŒƒæ•°
        }
    
    def compute_topological_invariants(self, x):
        """è®¡ç®—æ‹“æ‰‘ä¸å˜é‡ - å¢å¼ºç‰ˆæœ¬"""
        batch_size, n, d = x.shape
        all_invariants = {
            'first_chern_class': [], 'second_chern_class': [], 
            'trace_F': [], 'trace_F_squared': [],
            'chern_ratio': [], 'curvature_norm': []
        }
        
        for i in range(n):
            x_slice = x[:, i:i+1, :]  # [batch_size, 1, d_model]
            
            connection = self.compute_connection_form(x_slice)
            curvature = self.compute_curvature_form(connection)
            chern_classes = self.compute_chern_class(curvature)
            
            all_invariants['first_chern_class'].append(chern_classes['first_chern_class'])
            all_invariants['second_chern_class'].append(chern_classes['second_chern_class'])
            all_invariants['trace_F'].append(chern_classes['trace_F'])
            all_invariants['trace_F_squared'].append(chern_classes['trace_F_squared'])
            all_invariants['chern_ratio'].append(chern_classes['chern_ratio'])
            all_invariants['curvature_norm'].append(chern_classes['curvature_norm'])
        
        # åˆå¹¶æ‰€æœ‰ä½ç½®çš„ç»“æœ
        result = {}
        for key in all_invariants:
            if all_invariants[key]:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                result[key] = torch.stack(all_invariants[key], dim=1)
            else:
                # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé€‚å½“å½¢çŠ¶çš„é›¶å¼ é‡
                result[key] = torch.zeros(batch_size, n, dtype=torch.float32, device=x.device)
        
        return result
    
    def forward(self, x):
        """x: [batch_size, vocab_size, d_model]"""
        batch_size, n, d = x.shape
        
        # ä¿å­˜è¾“å…¥ç”¨äºé™ˆç±»è®¡ç®—
        self.activations['input'] = x.clone()
        self.activations['input_topology'] = self.compute_topological_invariants(x)
        
        # åº”ç”¨å‡ ä½•å˜æ¢
        x_flat = x.view(-1, d)
        for i, layer in enumerate(self.geometric_layers):
            x_flat = layer(x_flat)
            layer_output = x_flat.view(batch_size, n, d)
            
            self.activations[f'layer{i+1}'] = layer_output.clone()
            self.activations[f'layer{i+1}_topology'] = self.compute_topological_invariants(layer_output)
        
        x = x_flat.view(batch_size, n, d)
        self.activations['output'] = x.clone()
        self.activations['output_topology'] = self.compute_topological_invariants(x)
        
        return x

class EnhancedTopologicalClassifier:
    """å¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»å™¨ - æé«˜ç‰¹å¾åŒºåˆ†èƒ½åŠ›"""
    def __init__(self, n_classes=3):
        self.n_classes = n_classes
        self.kmeans = KMeans(n_clusters=n_classes, random_state=42)
        self.pca = PCA(n_components=2)
        self.scaler = StandardScaler()
        
        # å­˜å‚¨æ‹“æ‰‘ç‰¹å¾
        self.topological_features = []
        self.labels = []
        self.fitted = False
    
    def extract_enhanced_topological_features(self, model):
        """æå–å¢å¼ºç‰ˆæ‹“æ‰‘ç‰¹å¾"""
        features = []
        
        # æå–å„å±‚çš„å¢å¼ºæ‹“æ‰‘ä¸å˜é‡ä½œä¸ºç‰¹å¾
        for layer_key in ['input_topology', 'layer1_topology', 'layer2_topology', 'output_topology']:
            if layer_key in model.activations:
                topo_data = model.activations[layer_key]
                
                # æ„å»ºå¢å¼ºç‰¹å¾å‘é‡ - åŒ…å«æ›´å¤šæ‹“æ‰‘ä¸å˜é‡
                feature_vector = [
                    # åŸºç¡€é™ˆç±»ç‰¹å¾
                    torch.mean(topo_data['first_chern_class']).item(),
                    torch.std(topo_data['first_chern_class']).item(),
                    torch.mean(topo_data['second_chern_class']).item(),
                    torch.std(topo_data['second_chern_class']).item(),
                    torch.mean(topo_data['trace_F']).item(),
                    torch.std(topo_data['trace_F']).item(),
                    torch.mean(topo_data['trace_F_squared']).item(),
                    # å¢å¼ºç‰¹å¾
                    torch.mean(topo_data['chern_ratio']).item(),
                    torch.std(topo_data['chern_ratio']).item(),
                    torch.mean(topo_data['curvature_norm']).item(),
                    torch.std(topo_data['curvature_norm']).item(),
                    # é«˜é˜¶ç‰¹å¾
                    torch.mean(topo_data['first_chern_class']**2).item(),
                    torch.mean(topo_data['second_chern_class']**2).item(),
                    torch.mean(torch.abs(topo_data['trace_F'])).item(),
                    torch.mean(torch.abs(topo_data['trace_F_squared'])).item()
                ]
                
                features.extend(feature_vector)
        
        return np.array(features)
    
    def fit(self, models, labels=None):
        """è®­ç»ƒå¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»å™¨"""
        features_list = []
        
        for model in models:
            features = self.extract_enhanced_topological_features(model)
            features_list.append(features)
        
        features_array = np.array(features_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.fit_transform(features_array)
        
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä½¿ç”¨èšç±»
        if labels is None:
            self.labels = self.kmeans.fit_predict(features_scaled)
        else:
            self.labels = np.array(labels)
            self.kmeans.fit(features_scaled)  # æ˜¾å¼æ‹ŸåˆKMeans
        
        # é™ç»´ç”¨äºå¯è§†åŒ–
        self.features_2d = self.pca.fit_transform(features_scaled)
        self.fitted = True  # æ ‡è®°ä¸ºå·²æ‹Ÿåˆ
        
        return self
    
    def predict(self, models):
        """é¢„æµ‹æ¨¡å‹çš„æ‹“æ‰‘ç±»åˆ«"""
        if not self.fitted:
            raise ValueError("åˆ†ç±»å™¨å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        features_list = []
        
        for model in models:
            features = self.extract_enhanced_topological_features(model)
            features_list.append(features)
        
        features_array = np.array(features_list)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.transform(features_array)
        
        # ä½¿ç”¨K-meansè¿›è¡Œé¢„æµ‹
        predictions = self.kmeans.predict(features_scaled)
        
        # é™ç»´ç”¨äºå¯è§†åŒ–
        features_2d = self.pca.transform(features_scaled)
        
        return predictions, features_2d
    
    def visualize_classification(self, predictions=None, features_2d=None):
        """å¯è§†åŒ–æ‹“æ‰‘åˆ†ç±»ç»“æœ"""
        if predictions is None:
            predictions = self.labels
            features_2d = self.features_2d
        
        plt.figure(figsize=(12, 8))
        
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                           c=predictions, cmap='viridis', alpha=0.7, s=100)
        
        plt.colorbar(scatter)
        plt.title('å¢å¼ºç‰ˆè®¤çŸ¥ç³»ç»Ÿæ‹“æ‰‘åˆ†ç±» (PCAé™ç»´å¯è§†åŒ–)', fontsize=14)
        plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
        plt.ylabel('ç¬¬äºŒä¸»æˆåˆ†')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç±»åˆ«ä¸­å¿ƒ
        if hasattr(self, 'kmeans'):
            centers_2d = self.pca.transform(self.scaler.transform(self.kmeans.cluster_centers_))
            plt.scatter(centers_2d[:, 0], centers_2d[:, 1], 
                       c='red', marker='x', s=200, linewidths=3, label='ç±»åˆ«ä¸­å¿ƒ')
            plt.legend()
        
        plt.tight_layout()
        plt.show()
    
    def analyze_topological_patterns(self, models, labels):
        """åˆ†ææ‹“æ‰‘æ¨¡å¼"""
        print("ğŸ” å¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»åˆ†æ:")
        
        # æå–ç‰¹å¾
        features_list = []
        for model in models:
            features = self.extract_enhanced_topological_features(model)
            features_list.append(features)
        
        features_array = np.array(features_list)
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        unique_labels = np.unique(labels)
        for label in unique_labels:
            mask = labels == label
            class_features = features_array[mask]
            
            print(f"\n  ç±»åˆ« {label}:")
            print(f"    æ ·æœ¬æ•°é‡: {np.sum(mask)}")
            print(f"    ç¬¬ä¸€é™ˆç±»å‡å€¼: {np.mean(class_features[:, 0]):.6f} Â± {np.std(class_features[:, 0]):.6f}")
            print(f"    ç¬¬äºŒé™ˆç±»å‡å€¼: {np.mean(class_features[:, 2]):.6f} Â± {np.std(class_features[:, 2]):.6f}")
            print(f"    é™ˆç±»æ¯”å€¼å‡å€¼: {np.mean(class_features[:, 7]):.6f} Â± {np.std(class_features[:, 7]):.6f}")
            print(f"    æ›²ç‡èŒƒæ•°å‡å€¼: {np.mean(class_features[:, 9]):.6f} Â± {np.std(class_features[:, 9]):.6f}")

def create_enhanced_cognitive_systems_for_classification():
    """åˆ›å»ºå¢å¼ºç‰ˆä¸åŒç±»å‹çš„è®¤çŸ¥ç³»ç»Ÿç”¨äºåˆ†ç±»"""
    print("ğŸ§ª åˆ›å»ºå¢å¼ºç‰ˆä¸åŒç±»å‹çš„è®¤çŸ¥ç³»ç»Ÿ...")
    
    systems = []
    system_types = []
    
    batch_size = 1
    vocab_size = 8
    d_model = 16
    
    # ç±»å‹1: æ­£å¸¸è®¤çŸ¥ç³»ç»Ÿ
    for i in range(10):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        inputs = torch.randn(batch_size, vocab_size, d_model)
        _ = model(inputs)
        systems.append(model)
        system_types.append(0)  # æ­£å¸¸ç³»ç»Ÿ
    
    # ç±»å‹2: é«˜å¼‚å¸¸å€¼è®¤çŸ¥ç³»ç»Ÿ
    for i in range(10):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, 0] = np.random.uniform(50, 100)  # é«˜å¼‚å¸¸å€¼
        inputs[0, 1, 5] = np.random.uniform(-100, -50)  # å¯¹åº”è´Ÿå€¼
        _ = model(inputs)
        systems.append(model)
        system_types.append(1)  # å¼‚å¸¸ç³»ç»Ÿ
    
    # ç±»å‹3: çº¦æŸè¿åè®¤çŸ¥ç³»ç»Ÿ
    for i in range(10):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, :] = torch.ones(d_model) * np.random.uniform(20, 50)  # çº¦æŸè¿å
        inputs[0, 1, :] = torch.ones(d_model) * -np.random.uniform(20, 50)
        inputs[0, 2, :] = torch.ones(d_model) * np.random.uniform(10, 30)  # é¢å¤–çº¦æŸ
        _ = model(inputs)
        systems.append(model)
        system_types.append(2)  # çº¦æŸè¿åç³»ç»Ÿ
    
    return systems, system_types

def run_enhanced_topological_classification():
    """è¿è¡Œå¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»ç®—æ³•"""
    print("ğŸ” å¼€å§‹å¢å¼ºç‰ˆè®¤çŸ¥ç³»ç»Ÿçš„æ‹“æ‰‘åˆ†ç±»...")
    
    # åˆ›å»ºå¢å¼ºç‰ˆè®¤çŸ¥ç³»ç»Ÿ
    systems, true_types = create_enhanced_cognitive_systems_for_classification()
    
    print(f"  åˆ›å»ºäº† {len(systems)} ä¸ªå¢å¼ºç‰ˆè®¤çŸ¥ç³»ç»Ÿ")
    print(f"  åŒ…å« 3 ç§ç±»å‹: æ­£å¸¸ç³»ç»Ÿ(0), å¼‚å¸¸ç³»ç»Ÿ(1), çº¦æŸè¿åç³»ç»Ÿ(2)")
    
    # è®­ç»ƒå¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»å™¨
    classifier = EnhancedTopologicalClassifier(n_classes=3)
    classifier.fit(systems, true_types)
    
    # é¢„æµ‹
    predictions, features_2d = classifier.predict(systems)
    
    # åˆ†æç»“æœ
    print(f"\nğŸ“Š å¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»ç»“æœ:")
    accuracy = accuracy_score(true_types, predictions) * 100
    print(f"  é¢„æµ‹å‡†ç¡®ç‡: {accuracy:.1f}%")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_types, predictions)
    print(f"  æ··æ·†çŸ©é˜µ:")
    print(f"    çœŸå®\\é¢„æµ‹  0ç±»  1ç±»  2ç±»")
    for i, row in enumerate(cm):
        print(f"    {i}ç±»      {row[0]:3d}  {row[1]:3d}  {row[2]:3d}")
    
    # å¯è§†åŒ–
    classifier.visualize_classification(predictions, features_2d)
    
    # åˆ†ææ‹“æ‰‘æ¨¡å¼
    classifier.analyze_topological_patterns(systems, np.array(true_types))
    
    # æ‹“æ‰‘ç‰¹å¾é‡è¦æ€§åˆ†æ
    print(f"\nğŸ¯ å¢å¼ºç‰ˆæ‹“æ‰‘ç‰¹å¾é‡è¦æ€§åˆ†æ:")
    
    # è®¡ç®—å„ç±»åˆ«é—´çš„ç‰¹å¾å·®å¼‚
    systems_by_type = [[] for _ in range(3)]
    for i, sys_type in enumerate(true_types):
        systems_by_type[sys_type].append(systems[i])
    
    # è®¡ç®—æ¯ç±»çš„å¹³å‡ç‰¹å¾
    avg_features = []
    for type_idx, type_systems in enumerate(systems_by_type):
        type_features = []
        for sys in type_systems:
            features = classifier.extract_enhanced_topological_features(sys)
            type_features.append(features)
        avg_features.append(np.mean(type_features, axis=0))
    
    print("  å„ç±»å‹ç³»ç»Ÿçš„å¹³å‡æ‹“æ‰‘ç‰¹å¾ (å‰10ä¸ªç‰¹å¾):")
    feature_names = [
        "ç¬¬ä¸€é™ˆç±»å‡å€¼", "ç¬¬ä¸€é™ˆç±»æ ‡å‡†å·®", "ç¬¬äºŒé™ˆç±»å‡å€¼", "ç¬¬äºŒé™ˆç±»æ ‡å‡†å·®",
        "æ›²ç‡è¿¹å‡å€¼", "æ›²ç‡è¿¹æ ‡å‡†å·®", "æ›²ç‡å¹³æ–¹è¿¹å‡å€¼", "é™ˆç±»æ¯”å€¼å‡å€¼",
        "é™ˆç±»æ¯”å€¼æ ‡å‡†å·®", "æ›²ç‡èŒƒæ•°å‡å€¼"
    ]
    
    for i, name in enumerate(feature_names):
        print(f"    {name}: æ­£å¸¸={avg_features[0][i]:.6f}, å¼‚å¸¸={avg_features[1][i]:.6f}, çº¦æŸ={avg_features[2][i]:.6f}")
    
    print(f"\nğŸŒŸ å¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»çš„ç†è®ºæ„ä¹‰:")
    print(f"  â€¢ å¢å¼ºç‰¹å¾æå–æé«˜äº†ç³»ç»Ÿç±»å‹çš„åŒºåˆ†èƒ½åŠ›")
    print(f"  â€¢ é™ˆç±»æ¯”å€¼ç­‰é«˜é˜¶æ‹“æ‰‘ä¸å˜é‡æä¾›äº†æ›´å¤šåŒºåˆ†ä¿¡æ¯")
    print(f"  â€¢ æ›²ç‡èŒƒæ•°ç­‰å‡ ä½•é‡å¢å¼ºäº†æ‹“æ‰‘ç‰¹å¾çš„è¡¨è¾¾åŠ›")
    print(f"  â€¢ éªŒè¯äº†è®¤çŸ¥çº¤ç»´ä¸›ç†è®ºçš„é«˜é˜¶é¢„æµ‹")
    
    return classifier, systems, true_types, predictions

# è¿è¡Œå¢å¼ºç‰ˆæ‹“æ‰‘åˆ†ç±»
if __name__ == "__main__":
    classifier, systems, true_types, predictions = run_enhanced_topological_classification()
