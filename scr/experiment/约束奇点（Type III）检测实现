import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse.csgraph import connected_components

class SparseFiberBundle(nn.Module):
    """ç¨€ç–çº¤ç»´ä¸›è¿‘ä¼¼ç®—æ³• - æ”¯æŒçº¦æŸå¥‡ç‚¹æ£€æµ‹"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.k_neighbors = k_neighbors
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ç”¨äºå¥‡ç‚¹æ£€æµ‹
        self.activations = {}
        
        # çº¦æŸç³»ç»Ÿï¼šå­˜å‚¨é€»è¾‘çº¦æŸ
        self.constraint_system = nn.ParameterDict({
            'consistency_constraint': nn.Parameter(torch.tensor(1.0)),  # ä¸€è‡´æ€§çº¦æŸ
            'symmetry_constraint': nn.Parameter(torch.tensor(0.0)),     # å¯¹ç§°æ€§çº¦æŸ
            'transitivity_constraint': nn.Parameter(torch.tensor(0.0))  # ä¼ é€’æ€§çº¦æŸ
        })
        
        # çº¦æŸæ¿€æ´»å†å²
        self.constraint_history = {}
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            )
        ])
    
    def compute_constraint_violations(self, x):
        """è®¡ç®—çº¦æŸè¿åæƒ…å†µ"""
        batch_size, n, d = x.shape
        
        # ä¸€è‡´æ€§çº¦æŸï¼šç›¸é‚»å…ƒç´ å€¼åº”è¯¥ç›¸è¿‘
        consistency_violation = 0.0
        if n > 1:
            for i in range(n-1):
                consistency_violation += torch.mean(torch.abs(x[:, i, :] - x[:, i+1, :]))
        
        # å¯¹ç§°æ€§çº¦æŸï¼šx[i,j] = x[j,i] (å¦‚æœxæ˜¯å…³ç³»çŸ©é˜µ)
        symmetry_violation = 0.0
        if n > 2:
            x_flat = x.view(batch_size * n, d)
            similarity = torch.mm(x_flat, x_flat.t())
            # æ£€æŸ¥å¯¹ç§°æ€§
            symmetry_violation = torch.mean(torch.abs(similarity - similarity.t()))
        
        # ä¼ é€’æ€§çº¦æŸï¼šå¦‚æœaâ†’b, bâ†’cï¼Œåˆ™åº”è¯¥aâ†’c
        transitivity_violation = 0.0
        if n > 3:
            x_flat = x.view(batch_size * n, d)
            similarity = torch.mm(x_flat, x_flat.t())
            # ä¼ é€’æ€§ï¼šAÃ—BÃ—Cåº”æ¥è¿‘AÃ—C
            similarity_sq = torch.mm(similarity, similarity)
            transitivity_violation = torch.mean(torch.abs(similarity_sq - similarity))
        
        return {
            'consistency_violation': consistency_violation.item(),
            'symmetry_violation': symmetry_violation.item(),
            'transitivity_violation': transitivity_violation.item()
        }
    
    def compute_connectivity_matrix(self, x):
        """è®¡ç®—æ¿€æ´»æ¨¡å¼çš„è¿æ¥æ€§çŸ©é˜µ"""
        batch_size, n, d = x.shape
        x_flat = x.view(batch_size * n, d)
        similarity = torch.mm(x_flat, x_flat.t()) / (
            torch.norm(x_flat, dim=1, keepdim=True) * torch.norm(x_flat, dim=1, keepdim=True).t() + 1e-8
        )
        return torch.clamp(similarity, min=0.0)
    
    def compute_topological_features(self, x):
        """è®¡ç®—æ‹“æ‰‘ç‰¹å¾"""
        connectivity = self.compute_connectivity_matrix(x)
        batch_size = x.shape[0]
        
        connectivity_np = connectivity.detach().cpu().numpy()
        n_components, labels = connected_components(
            csgraph=connectivity_np, 
            directed=False, 
            return_labels=True
        )
        
        return {
            'n_components': n_components,
            'connectivity': connectivity,
            'labels': torch.tensor(labels).to(x.device),
            'density': torch.mean(connectivity).item()
        }
    
    def forward(self, x):
        """x: [batch_size, vocab_size, d_model]"""
        batch_size, n, d = x.shape
        x_flat = x.view(-1, d)
        
        # ä¿å­˜è¾“å…¥ç”¨äºå¥‡ç‚¹æ£€æµ‹
        self.activations['input'] = x.clone()
        self.activations['input_topology'] = self.compute_topological_features(x)
        self.activations['input_constraints'] = self.compute_constraint_violations(x)
        
        # åº”ç”¨å‡ ä½•å˜æ¢
        for i, layer in enumerate(self.geometric_layers):
            x_flat = layer(x_flat)
            layer_output = x_flat.view(batch_size, n, d)
            
            self.activations[f'layer{i+1}'] = layer_output.clone()
            self.activations[f'layer{i+1}_topology'] = self.compute_topological_features(layer_output)
            self.activations[f'layer{i+1}_constraints'] = self.compute_constraint_violations(layer_output)
        
        x = x_flat.view(batch_size, n, d)
        self.activations['output'] = x.clone()
        self.activations['output_topology'] = self.compute_topological_features(x)
        self.activations['output_constraints'] = self.compute_constraint_violations(x)
        
        return x

class SingularityDrivenLearner:
    """å¥‡ç‚¹é©±åŠ¨å­¦ä¹ ç®—æ³• - æ”¯æŒType I, II, IIIå¥‡ç‚¹"""
    def __init__(self, model):
        self.model = model
        self.singularity_history = []
        self.singularity_threshold = 2.0  # é€»è¾‘å¥‡ç‚¹é˜ˆå€¼
        self.topology_change_threshold = 0.5  # æ‹“æ‰‘å˜åŒ–é˜ˆå€¼
        self.constraint_violation_threshold = 1.0  # çº¦æŸè¿åé˜ˆå€¼
    
    def detect_type_I_singularities(self):
        """æ£€æµ‹Type Iå¥‡ç‚¹ï¼ˆé€»è¾‘å¥‡ç‚¹ï¼‰"""
        detected_singularities = []
        
        for layer_name, activation in self.model.activations.items():
            if 'topology' not in layer_name and 'constraints' not in layer_name:
                max_abs = torch.max(torch.abs(activation))
                if max_abs > self.singularity_threshold:
                    positions = torch.where(torch.abs(activation) > self.singularity_threshold)
                    detected_singularities.append({
                        'type': 'Type_I',
                        'layer': layer_name,
                        'detected': True,
                        'value': max_abs.item(),
                        'positions': positions,
                        'count': len(positions[0])
                    })
        
        return detected_singularities
    
    def detect_type_II_singularities(self):
        """æ£€æµ‹Type IIå¥‡ç‚¹ï¼ˆæ‹“æ‰‘å¥‡ç‚¹ï¼‰"""
        detected_singularities = []
        
        topology_keys = [k for k in self.model.activations.keys() if 'topology' in k]
        
        for i in range(len(topology_keys)-1):
            current_key = topology_keys[i]
            next_key = topology_keys[i+1]
            
            current_topo = self.model.activations[current_key]
            next_topo = self.model.activations[next_key]
            
            comp_change = abs(current_topo['n_components'] - next_topo['n_components'])
            density_change = abs(current_topo['density'] - next_topo['density'])
            
            if comp_change > 0 or density_change > self.topology_change_threshold:
                detected_singularities.append({
                    'type': 'Type_II',
                    'layer_transition': f"{current_key} -> {next_key}",
                    'detected': True,
                    'n_components_change': comp_change,
                    'density_change': density_change,
                    'current_components': current_topo['n_components'],
                    'next_components': next_topo['n_components']
                })
        
        return detected_singularities
    
    def detect_type_III_singularities(self):
        """æ£€æµ‹Type IIIå¥‡ç‚¹ï¼ˆçº¦æŸå¥‡ç‚¹ï¼‰"""
        detected_singularities = []
        
        constraint_keys = [k for k in self.model.activations.keys() if 'constraints' in k]
        
        for key in constraint_keys:
            constraints = self.model.activations[key]
            
            # æ£€æŸ¥å„ç§çº¦æŸè¿å
            if constraints['consistency_violation'] > self.constraint_violation_threshold:
                detected_singularities.append({
                    'type': 'Type_III',
                    'layer': key,
                    'constraint_type': 'consistency',
                    'violation_value': constraints['consistency_violation'],
                    'detected': True
                })
            
            if constraints['symmetry_violation'] > self.constraint_violation_threshold:
                detected_singularities.append({
                    'type': 'Type_III',
                    'layer': key,
                    'constraint_type': 'symmetry',
                    'violation_value': constraints['symmetry_violation'],
                    'detected': True
                })
            
            if constraints['transitivity_violation'] > self.constraint_violation_threshold:
                detected_singularities.append({
                    'type': 'Type_III',
                    'layer': key,
                    'constraint_type': 'transitivity',
                    'violation_value': constraints['transitivity_violation'],
                    'detected': True
                })
        
        return detected_singularities
    
    def apply_repair_to_tensor(self, tensor):
        """å¯¹å¼ é‡åº”ç”¨ä¿®å¤ç­–ç•¥"""
        repaired = torch.clamp(tensor, -1.0, 1.0)
        return repaired
    
    def apply_constraint_repair(self, tensor, constraint_violations):
        """åº”ç”¨çº¦æŸä¿®å¤ç­–ç•¥"""
        batch_size, n, d = tensor.shape
        
        # æ ¹æ®çº¦æŸè¿åç±»å‹åº”ç”¨ä¸åŒä¿®å¤ç­–ç•¥
        if constraint_violations.get('consistency_violation', 0) > self.constraint_violation_threshold:
            # ä¸€è‡´æ€§ä¿®å¤ï¼šå¹³æ»‘ç›¸é‚»å…ƒç´ 
            for i in range(n-1):
                tensor[:, i, :] = 0.5 * (tensor[:, i, :] + tensor[:, i+1, :])
        
        if constraint_violations.get('symmetry_violation', 0) > self.constraint_violation_threshold:
            # å¯¹ç§°æ€§ä¿®å¤ï¼šç¡®ä¿å¯¹ç§°æ€§
            x_flat = tensor.view(batch_size * n, d)
            similarity = torch.mm(x_flat, x_flat.t())
            # å¼ºåˆ¶å¯¹ç§°æ€§
            sym_similarity = 0.5 * (similarity + similarity.t())
            # åå‘æŠ•å½±ï¼ˆç®€åŒ–ç‰ˆï¼‰
            tensor = torch.mean(tensor, dim=1, keepdim=True).expand_as(tensor)
        
        return tensor
    
    def train_step_with_comprehensive_repair(self, batch):
        """å…¨é¢ä¿®å¤æ¨¡å¼ï¼šæ£€æµ‹å¹¶ä¿®å¤æ‰€æœ‰ç±»å‹å¥‡ç‚¹"""
        # 1. å‰å‘ä¼ æ’­
        outputs = self.model(batch)
        
        # 2. æ£€æµ‹æ‰€æœ‰ç±»å‹å¥‡ç‚¹
        type_I_singularities = self.detect_type_I_singularities()
        type_II_singularities = self.detect_type_II_singularities()
        type_III_singularities = self.detect_type_III_singularities()
        
        # 3. åˆå¹¶æ‰€æœ‰å¥‡ç‚¹
        all_singularities = type_I_singularities + type_II_singularities + type_III_singularities
        
        # 4. åº”ç”¨ä¿®å¤
        repaired_outputs = outputs.clone()
        
        if type_I_singularities:
            repaired_outputs = self.apply_repair_to_tensor(repaired_outputs)
        
        if type_III_singularities:
            # è·å–è¾“å‡ºå±‚çº¦æŸ
            output_constraints = self.model.activations['output_constraints']
            repaired_outputs = self.apply_constraint_repair(repaired_outputs, output_constraints)
        
        # 5. è®°å½•å¥‡ç‚¹å†å²
        self.singularity_history.extend(all_singularities)
        
        # 6. è®¡ç®—æŸå¤±
        loss = torch.mean((repaired_outputs - batch) ** 2)
        
        # 7. å¥‡ç‚¹å¥–åŠ±æœºåˆ¶
        if all_singularities:
            reward = 0.1 * len(all_singularities)
            loss = loss - reward
        
        return repaired_outputs, loss, all_singularities

def visualize_all_types_singularity_detection():
    """å¯è§†åŒ–æ‰€æœ‰ç±»å‹å¥‡ç‚¹æ£€æµ‹"""
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 1
    vocab_size = 10
    d_model = 64
    
    # 1. åˆ›å»ºæ¨¡å‹
    model = SparseFiberBundle(vocab_size, d_model=d_model)
    learner = SingularityDrivenLearner(model)
    
    # 2. åˆ›å»ºåŒ…å«å„ç§å¼‚å¸¸çš„è¾“å…¥
    inputs = torch.randn(batch_size, vocab_size, d_model)
    
    # åˆ¶é€ ä¸‰ç§å¼‚å¸¸ï¼š
    # Type I: é€»è¾‘å¼‚å¸¸
    inputs[0, 0, 0] = 10.0
    inputs[0, 1, 1] = -15.0
    
    # Type II: æ‹“æ‰‘å¼‚å¸¸ï¼ˆé€šè¿‡æç«¯å€¼å½±å“è¿æ¥æ€§ï¼‰
    inputs[0, 2, :] = 20.0  # é«˜æ¿€æ´»å€¼
    inputs[0, 3, :] = -20.0  # é«˜æ¿€æ´»å€¼
    
    # Type III: çº¦æŸå¼‚å¸¸ï¼ˆç ´åé€»è¾‘çº¦æŸï¼‰
    inputs[0, 4, :] = torch.ones(d_model) * 10  # ä¸€è‡´æ€§ç ´å
    inputs[0, 5, :] = torch.ones(d_model) * -10 # ä¸€è‡´æ€§ç ´å
    
    print("ğŸ” åˆ¶é€ çš„å¼‚å¸¸å€¼:")
    print(f"  Type I (é€»è¾‘): ä½ç½®(0,0,0)={inputs[0, 0, 0].item():.2f}, (0,1,1)={inputs[0, 1, 1].item():.2f}")
    print(f"  Type II (æ‹“æ‰‘): ä½ç½®(0,2,:)å’Œ(0,3,:)ï¼ˆé«˜æ¿€æ´»å€¼ï¼‰")
    print(f"  Type III (çº¦æŸ): ä½ç½®(0,4,:)å’Œ(0,5,:)ï¼ˆç ´åä¸€è‡´æ€§ï¼‰")
    
    # 3. å…¨é¢ä¿®å¤è®­ç»ƒ
    outputs, loss, detected_singularities = learner.train_step_with_comprehensive_repair(inputs)
    
    # 4. è·å–æ‹“æ‰‘ä¿¡æ¯ç”¨äºåç»­åˆ†æ
    input_topo_info = model.activations['input_topology']
    output_topo_info = model.activations['output_topology']
    
    # 5. å¯è§†åŒ–
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    axes = axes.flatten()
    
    # Type I: é€»è¾‘å¼‚å¸¸
    input_flat = inputs[0].view(-1).detach().numpy()
    axes[0].hist(input_flat, bins=50, alpha=0.7, color='red', edgecolor='black')
    axes[0].set_title('è¾“å…¥æ•°æ®åˆ†å¸ƒ (å«é€»è¾‘å¼‚å¸¸)', fontsize=12)
    axes[0].set_xlabel('å€¼')
    axes[0].set_ylabel('é¢‘æ¬¡')
    axes[0].axvline(x=learner.singularity_threshold, color='orange', linestyle='--', label='é€»è¾‘é˜ˆå€¼')
    axes[0].axvline(x=-learner.singularity_threshold, color='orange', linestyle='--')
    axes[0].legend()
    
    output_flat = outputs[0].view(-1).detach().numpy()
    axes[1].hist(output_flat, bins=50, alpha=0.7, color='blue', edgecolor='black')
    axes[1].set_title('è¾“å‡ºæ•°æ®åˆ†å¸ƒ (ä¿®å¤å)', fontsize=12)
    axes[1].set_xlabel('å€¼')
    axes[1].set_ylabel('é¢‘æ¬¡')
    axes[1].axvline(x=1, color='green', linestyle='--', label='ä¿®å¤è¾¹ç•Œ')
    axes[1].axvline(x=-1, color='green', linestyle='--')
    axes[1].legend()
    
    # Type II: æ‹“æ‰‘å¼‚å¸¸
    axes[2].bar(['è¾“å…¥å±‚', 'è¾“å‡ºå±‚'], 
                [input_topo_info['n_components'], output_topo_info['n_components']], 
                color=['red', 'blue'], alpha=0.7)
    axes[2].set_title(f'æ‹“æ‰‘è¿é€šåˆ†é‡æ•°\nè¾“å…¥: {input_topo_info["n_components"]}, è¾“å‡º: {output_topo_info["n_components"]}', 
                     fontsize=12)
    axes[2].set_ylabel('è¿é€šåˆ†é‡æ•°')
    
    axes[3].bar(['è¾“å…¥å±‚', 'è¾“å‡ºå±‚'], 
                [input_topo_info['density'], output_topo_info['density']], 
                color=['red', 'blue'], alpha=0.7)
    axes[3].set_title(f'è¿æ¥å¯†åº¦\nè¾“å…¥: {input_topo_info["density"]:.3f}, è¾“å‡º: {output_topo_info["density"]:.3f}', 
                     fontsize=12)
    axes[3].set_ylabel('è¿æ¥å¯†åº¦')
    
    # Type III: çº¦æŸå¼‚å¸¸
    input_constraints = model.activations['input_constraints']
    output_constraints = model.activations['output_constraints']
    
    constraint_names = ['ä¸€è‡´æ€§', 'å¯¹ç§°æ€§', 'ä¼ é€’æ€§']
    input_violations = [
        input_constraints['consistency_violation'],
        input_constraints['symmetry_violation'], 
        input_constraints['transitivity_violation']
    ]
    output_violations = [
        output_constraints['consistency_violation'],
        output_constraints['symmetry_violation'],
        output_constraints['transitivity_violation']
    ]
    
    x = np.arange(len(constraint_names))
    width = 0.35
    axes[4].bar(x - width/2, input_violations, width, label='ä¿®å¤å‰', alpha=0.7, color='red')
    axes[4].bar(x + width/2, output_violations, width, label='ä¿®å¤å', alpha=0.7, color='blue')
    axes[4].set_title('çº¦æŸè¿åç¨‹åº¦å¯¹æ¯”', fontsize=12)
    axes[4].set_xlabel('çº¦æŸç±»å‹')
    axes[4].set_ylabel('è¿åç¨‹åº¦')
    axes[4].set_xticks(x)
    axes[4].set_xticklabels(constraint_names)
    axes[4].legend()
    
    # ä¿®å¤å‰åå¯¹æ¯”
    n_compare = min(100, input_flat.shape[0])
    x_pos = np.arange(n_compare)
    
    axes[5].plot(x_pos, input_flat[:n_compare], 'ro-', alpha=0.6, label='ä¿®å¤å‰', markersize=4)
    axes[5].plot(x_pos, output_flat[:n_compare], 'bo-', alpha=0.6, label='ä¿®å¤å', markersize=4)
    axes[5].set_title('ä¿®å¤å‰åå¯¹æ¯” (å‰100ä¸ªå€¼)', fontsize=12)
    axes[5].set_xlabel('å…ƒç´ ç´¢å¼•')
    axes[5].set_ylabel('å€¼')
    axes[5].legend()
    axes[5].grid(True, alpha=0.3)
    
    # å¥‡ç‚¹ç±»å‹ç»Ÿè®¡
    type_I_count = len([s for s in detected_singularities if s['type'] == 'Type_I'])
    type_II_count = len([s for s in detected_singularities if s['type'] == 'Type_II'])
    type_III_count = len([s for s in detected_singularities if s['type'] == 'Type_III'])
    
    if type_I_count + type_II_count + type_III_count > 0:
        bars = axes[6].bar(['Type I (é€»è¾‘)', 'Type II (æ‹“æ‰‘)', 'Type III (çº¦æŸ)'], 
                          [type_I_count, type_II_count, type_III_count], 
                          color=['orange', 'purple', 'green'], alpha=0.7)
        axes[6].set_title(f'æ£€æµ‹åˆ°çš„å¥‡ç‚¹ç±»å‹\næ€»è®¡: {len(detected_singularities)}ä¸ª', fontsize=12)
        axes[6].set_ylabel('æ•°é‡')
        
        for bar, count in zip(bars, [type_I_count, type_II_count, type_III_count]):
            axes[6].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                        f'{count}', ha='center', va='bottom', fontsize=12)
    else:
        axes[6].text(0.5, 0.5, 'âœ… æœªæ£€æµ‹åˆ°å¥‡ç‚¹\n(å·²æˆåŠŸä¿®å¤)', 
                    horizontalalignment='center', verticalalignment='center',
                    transform=axes[6].transAxes, fontsize=14)
        axes[6].set_title('å¥‡ç‚¹æ£€æµ‹ç»“æœ', fontsize=12)
        axes[6].set_xlim(0, 1)
        axes[6].set_ylim(0, 1)
    
    # çº¦æŸä¿®å¤æ•ˆæœ
    constraint_types = ['consistency', 'symmetry', 'transitivity']
    input_violations_by_type = []
    output_violations_by_type = []
    
    for ctype in constraint_types:
        input_violations_by_type.append(input_constraints[f'{ctype}_violation'])
        output_violations_by_type.append(output_constraints[f'{ctype}_violation'])
    
    x = np.arange(len(constraint_types))
    axes[7].plot(x, input_violations_by_type, 'ro-', label='ä¿®å¤å‰', linewidth=2)
    axes[7].plot(x, output_violations_by_type, 'bo-', label='ä¿®å¤å', linewidth=2)
    axes[7].set_title('å„çº¦æŸç±»å‹ä¿®å¤æ•ˆæœ', fontsize=12)
    axes[7].set_xlabel('çº¦æŸç±»å‹')
    axes[7].set_ylabel('è¿åç¨‹åº¦')
    axes[7].set_xticks(x)
    axes[7].set_xticklabels(['ä¸€è‡´æ€§', 'å¯¹ç§°æ€§', 'ä¼ é€’æ€§'])
    axes[7].legend()
    axes[7].grid(True, alpha=0.3)
    
    # æ‹“æ‰‘ç¨³å®šæ€§
    topo_stable = abs(input_topo_info['n_components'] - output_topo_info['n_components']) <= 2
    stability_text = 'âœ… ç¨³å®š' if topo_stable else 'âš ï¸ å˜åŒ–'
    axes[8].text(0.5, 0.5, f'æ‹“æ‰‘ç¨³å®šæ€§\n{stability_text}', 
                horizontalalignment='center', verticalalignment='center',
                transform=axes[8].transAxes, fontsize=14, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen" if topo_stable else "lightcoral"))
    axes[8].set_title('æ‹“æ‰‘ç¨³å®šæ€§è¯„ä¼°', fontsize=12)
    axes[8].set_xlim(0, 1)
    axes[8].set_ylim(0, 1)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print(f"\nâœ… å…¨é¢ä¿®å¤ç»“æœ:")
    print(f"  è®­ç»ƒæŸå¤±: {loss.item():.4f}")
    print(f"  æ£€æµ‹åˆ°å¥‡ç‚¹æ€»æ•°: {len(detected_singularities)}")
    print(f"  Type Iå¥‡ç‚¹æ•°é‡: {type_I_count}")
    print(f"  Type IIå¥‡ç‚¹æ•°é‡: {type_II_count}")
    print(f"  Type IIIå¥‡ç‚¹æ•°é‡: {type_III_count}")
    
    if detected_singularities:
        print(f"  è¯¦ç»†å¥‡ç‚¹ä¿¡æ¯:")
        for i, sing in enumerate(detected_singularities):
            if sing['type'] == 'Type_I':
                print(f"    [{i+1}] Type I: å±‚={sing['layer']}, å€¼={sing['value']:.2f}, æ•°é‡={sing['count']}ä¸ª")
            elif sing['type'] == 'Type_II':
                print(f"    [{i+1}] Type II: {sing['layer_transition']}, "
                      f"è¿é€šåˆ†é‡å˜åŒ–={sing['n_components_change']}, "
                      f"å¯†åº¦å˜åŒ–={sing['density_change']:.3f}")
            else:  # Type III
                print(f"    [{i+1}] Type III: å±‚={sing['layer']}, "
                      f"çº¦æŸç±»å‹={sing['constraint_type']}, "
                      f"è¿åå€¼={sing['violation_value']:.3f}")
    
    # éªŒè¯ä¿®å¤æ•ˆæœ
    input_max = torch.max(torch.abs(inputs)).item()
    output_max = torch.max(torch.abs(outputs)).item()
    print(f"  ä¿®å¤å‰æœ€å¤§ç»å¯¹å€¼: {input_max:.2f}")
    print(f"  ä¿®å¤åæœ€å¤§ç»å¯¹å€¼: {output_max:.2f}")
    print(f"  ä¿®å¤æ•ˆæœ: {'âœ… æˆåŠŸ' if output_max <= 1.0 else 'âš ï¸ éƒ¨åˆ†ä¿®å¤'}")
    
    # æ‹“æ‰‘ç¨³å®šæ€§åˆ†æ
    topo_stable = abs(input_topo_info['n_components'] - output_topo_info['n_components']) <= 2
    print(f"  æ‹“æ‰‘ç¨³å®šæ€§: {'âœ… ç¨³å®š' if topo_stable else 'âš ï¸ å˜åŒ–è¾ƒå¤§'}")

# è¿è¡Œå…¨é¢å¯è§†åŒ–
if __name__ == "__main__":
    visualize_all_types_singularity_detection()
