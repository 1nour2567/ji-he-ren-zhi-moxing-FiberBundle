import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import math

class CognitiveFiberBundle(nn.Module):
    """è®¤çŸ¥çº¤ç»´ä¸›æ¨¡å‹ - é™ˆç±»è®¡ç®—çš„æœ€ç»ˆå®ç°"""
    def __init__(self, vocab_size, d_model=64, k_neighbors=8):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.k_neighbors = k_neighbors
        
        # å­˜å‚¨ä¸­é—´æ¿€æ´»å€¼ç”¨äºé™ˆç±»è®¡ç®—
        self.activations = {}
        
        # è®¤çŸ¥çº¤ç»´ä¸›çš„è”ç»œå½¢å¼ï¼ˆconnection formï¼‰
        self.connection_form = nn.Parameter(torch.randn(d_model, d_model) * 0.1)
        
        # ç®€åŒ–çš„æ›²ç‡å½¢å¼è®¡ç®—å‚æ•°
        self.curvature_weight = nn.Parameter(torch.randn(d_model, d_model) * 0.01)
        
        # å‡ ä½•ç¼–ç å±‚
        self.geometric_layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            ),
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU()
            )
        ])
    
    def compute_connection_form(self, x):
        """è®¡ç®—è”ç»œå½¢å¼ A - ä¿®å¤ç‰ˆæœ¬"""
        batch_size, n, d = x.shape
        x_flat = x.view(batch_size * n, d)
        
        # æ‰©å±•connection_formåˆ°batchç»´åº¦
        connection = self.connection_form.unsqueeze(0).expand(batch_size * n, -1, -1)
        
        # ç®€åŒ–ï¼šä¸ºæ¯ä¸ªä½ç½®æ·»åŠ åŸºäºè¾“å…¥çš„å¯¹è§’æ‰°åŠ¨
        input_diag = torch.diag_embed(torch.mean(x_flat, dim=1, keepdim=True).expand(-1, d) * 0.01)
        connection = connection + input_diag
        
        return connection
    
    def compute_curvature_form(self, connection):
        """è®¡ç®—æ›²ç‡å½¢å¼ F = dA + Aâˆ§A (ç®€åŒ–ç‰ˆæœ¬)"""
        batch_size_n, d, d = connection.shape
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šF = A*A - A^T*A (ææ‹¬å·çš„å¯¹è§’å ä¼˜è¿‘ä¼¼)
        A_squared = torch.bmm(connection, connection)
        A_transposed = connection.transpose(-2, -1)
        A_transposed_squared = torch.bmm(A_transposed, connection)
        
        curvature = A_squared - A_transposed_squared
        return curvature
    
    def compute_chern_class(self, curvature):
        """è®¡ç®—é™ˆç±» - æ‹“æ‰‘ä¸å˜é‡"""
        batch_size, d, d = curvature.shape
        
        # ç¬¬ä¸€é™ˆç±»: c1 = tr(F) / (2Ï€i)
        trace_F = torch.diagonal(curvature, dim1=-2, dim2=-1).sum(dim=-1)  # tr(F)
        first_chern_class = trace_F / (2 * math.pi * 1j)  # å¤æ•°å½¢å¼
        
        # ç¬¬äºŒé™ˆç±»: c2 = (tr(FÂ²) - tr(F)Â²) / (8Ï€Â²)
        F_squared = torch.bmm(curvature, curvature)
        trace_F_squared = torch.diagonal(F_squared, dim1=-2, dim2=-1).sum(dim=-1)  # tr(FÂ²)
        
        second_chern_class = (trace_F_squared - trace_F**2) / (8 * math.pi**2)
        
        return {
            'first_chern_class': first_chern_class.real,  # å–å®éƒ¨
            'second_chern_class': second_chern_class,
            'trace_F': trace_F,
            'trace_F_squared': trace_F_squared
        }
    
    def compute_topological_invariants(self, x):
        """è®¡ç®—æ‹“æ‰‘ä¸å˜é‡ - ä¿®å¤ç‰ˆæœ¬"""
        batch_size, n, d = x.shape
        all_invariants = {'first_chern_class': [], 'second_chern_class': [], 
                         'trace_F': [], 'trace_F_squared': []}
        
        for i in range(n):
            x_slice = x[:, i:i+1, :]  # [batch_size, 1, d_model]
            
            connection = self.compute_connection_form(x_slice)
            curvature = self.compute_curvature_form(connection)
            chern_classes = self.compute_chern_class(curvature)
            
            all_invariants['first_chern_class'].append(chern_classes['first_chern_class'])
            all_invariants['second_chern_class'].append(chern_classes['second_chern_class'])
            all_invariants['trace_F'].append(chern_classes['trace_F'])
            all_invariants['trace_F_squared'].append(chern_classes['trace_F_squared'])
        
        # åˆå¹¶æ‰€æœ‰ä½ç½®çš„ç»“æœ
        result = {}
        for key in all_invariants:
            if all_invariants[key]:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                result[key] = torch.stack(all_invariants[key], dim=1)
            else:
                # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé€‚å½“å½¢çŠ¶çš„é›¶å¼ é‡
                result[key] = torch.zeros(batch_size, n, dtype=torch.float32, device=x.device)
        
        return result
    
    def forward(self, x):
        """x: [batch_size, vocab_size, d_model]"""
        batch_size, n, d = x.shape
        
        # ä¿å­˜è¾“å…¥ç”¨äºé™ˆç±»è®¡ç®—
        self.activations['input'] = x.clone()
        self.activations['input_topology'] = self.compute_topological_invariants(x)
        
        # åº”ç”¨å‡ ä½•å˜æ¢
        x_flat = x.view(-1, d)
        for i, layer in enumerate(self.geometric_layers):
            x_flat = layer(x_flat)
            layer_output = x_flat.view(batch_size, n, d)
            
            self.activations[f'layer{i+1}'] = layer_output.clone()
            self.activations[f'layer{i+1}_topology'] = self.compute_topological_invariants(layer_output)
        
        x = x_flat.view(batch_size, n, d)
        self.activations['output'] = x.clone()
        self.activations['output_topology'] = self.compute_topological_invariants(x)
        
        return x

class ChernClassAnalyzer:
    """é™ˆç±»åˆ†æå™¨"""
    def __init__(self, model):
        self.model = model
        self.chern_history = []
    
    def analyze_chern_classes(self):
        """åˆ†æå„å±‚çš„é™ˆç±»"""
        topology_keys = [k for k in self.model.activations.keys() if 'topology' in k]
        
        chern_analysis = {}
        
        for key in topology_keys:
            topo_info = self.model.activations[key]
            
            chern_analysis[key] = {
                'first_chern_mean': torch.mean(topo_info['first_chern_class']).item(),
                'first_chern_std': torch.std(topo_info['first_chern_class']).item(),
                'second_chern_mean': torch.mean(topo_info['second_chern_class']).item(),
                'second_chern_std': torch.std(topo_info['second_chern_class']).item(),
                'trace_F_mean': torch.mean(topo_info['trace_F']).item(),
                'trace_F_std': torch.std(topo_info['trace_F']).item()
            }
        
        return chern_analysis
    
    def detect_topological_phase_transitions(self):
        """æ£€æµ‹æ‹“æ‰‘ç›¸å˜ï¼ˆé™ˆç±»çš„çªå˜ï¼‰"""
        topology_keys = [k for k in self.model.activations.keys() if 'topology' in k]
        
        phase_transitions = []
        
        for i in range(len(topology_keys)-1):
            current_key = topology_keys[i]
            next_key = topology_keys[i+1]
            
            current_chern = self.model.activations[current_key]
            next_chern = self.model.activations[next_key]
            
            # è®¡ç®—é™ˆç±»å˜åŒ–
            first_chern_change = torch.mean(torch.abs(
                torch.mean(current_chern['first_chern_class'], dim=[0,1]) - 
                torch.mean(next_chern['first_chern_class'], dim=[0,1])
            )).item()
            
            second_chern_change = torch.mean(torch.abs(
                torch.mean(current_chern['second_chern_class'], dim=[0,1]) - 
                torch.mean(next_chern['second_chern_class'], dim=[0,1])
            )).item()
            
            # å¦‚æœé™ˆç±»å˜åŒ–è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯æ‹“æ‰‘ç›¸å˜
            if first_chern_change > 0.1 or second_chern_change > 0.01:
                phase_transitions.append({
                    'transition': f"{current_key} -> {next_key}",
                    'first_chern_change': first_chern_change,
                    'second_chern_change': second_chern_change,
                    'detected': True
                })
        
        return phase_transitions

def find_topological_phase_transition_thresholds():
    """å¯»æ‰¾æ‹“æ‰‘ç›¸å˜çš„ä¸´ç•Œæ¡ä»¶"""
    print("ğŸ” å¯»æ‰¾æ‹“æ‰‘ç›¸å˜çš„ä¸´ç•Œæ¡ä»¶...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 1
    vocab_size = 8
    d_model = 16  # å‡å°ç»´åº¦ä»¥ç®€åŒ–è®¡ç®—
    
    # æµ‹è¯•ä¸åŒå¼ºåº¦çš„å¼‚å¸¸
    anomaly_strengths = np.logspace(0, 3, 20)  # ä»1åˆ°1000ï¼Œ20ä¸ªç‚¹
    phase_transitions_count = []
    chern_changes = []
    
    print("ğŸ§ª æµ‹è¯•å¼‚å¸¸å¼ºåº¦ä¸æ‹“æ‰‘ç›¸å˜çš„å…³ç³»:")
    
    for i, strength in enumerate(anomaly_strengths):
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        analyzer = ChernClassAnalyzer(model)
        
        # åˆ›å»ºå¼‚å¸¸è¾“å…¥ï¼Œå¼ºåº¦é€’å¢
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, 0] = strength    # é€’å¢å¼‚å¸¸å€¼
        inputs[0, 1, 0] = -strength  # å¯¹åº”è´Ÿå€¼
        
        outputs = model(inputs)
        chern_analysis = analyzer.analyze_chern_classes()
        phase_transitions = analyzer.detect_topological_phase_transitions()
        
        # è®¡ç®—ç¬¬äºŒé™ˆç±»å˜åŒ–
        chern_change = abs(chern_analysis['input_topology']['second_chern_mean'] - 
                          chern_analysis['output_topology']['second_chern_mean'])
        
        phase_transitions_count.append(len(phase_transitions))
        chern_changes.append(chern_change)
        
        if len(phase_transitions) > 0 or chern_change > 0.01:
            print(f"  å¼‚å¸¸å¼ºåº¦={strength:.1f}: æ‹“æ‰‘ç›¸å˜={len(phase_transitions)}ä¸ª, é™ˆç±»å˜åŒ–={chern_change:.6f}")
    
    # å¯»æ‰¾ä¸´ç•Œç‚¹
    critical_points = []
    for i in range(1, len(phase_transitions_count)):
        if phase_transitions_count[i] > phase_transitions_count[i-1] or chern_changes[i] > 0.01:
            critical_points.append((anomaly_strengths[i], chern_changes[i], phase_transitions_count[i]))
    
    print(f"\nğŸ¯ å‘ç°ä¸´ç•Œç‚¹: {len(critical_points)}ä¸ª")
    for i, (strength, chern_change, trans_count) in enumerate(critical_points):
        print(f"  ä¸´ç•Œç‚¹{i+1}: å¼‚å¸¸å¼ºåº¦={strength:.1f}, é™ˆç±»å˜åŒ–={chern_change:.6f}, æ‹“æ‰‘ç›¸å˜={trans_count}ä¸ª")
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„å¼‚å¸¸
    print(f"\nğŸ§ª æµ‹è¯•ä¸åŒç±»å‹çš„å¼‚å¸¸ä¸´ç•Œæ¡ä»¶:")
    
    # ç±»å‹1: å•ç‚¹å¼‚å¸¸
    print("  1. å•ç‚¹å¼‚å¸¸ä¸´ç•Œæ¡ä»¶:")
    single_anomaly_threshold = None
    for strength in np.logspace(1, 4, 50):  # ä»10åˆ°10000
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        analyzer = ChernClassAnalyzer(model)
        
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, 0] = strength  # å•ç‚¹å¼‚å¸¸
        
        outputs = model(inputs)
        phase_transitions = analyzer.detect_topological_phase_transitions()
        
        if len(phase_transitions) > 0:
            single_anomaly_threshold = strength
            print(f"    ä¸´ç•Œå¼ºåº¦: {strength:.1f}")
            break
    
    # ç±»å‹2: å¤šç‚¹å¼‚å¸¸
    print("  2. å¤šç‚¹å¼‚å¸¸ä¸´ç•Œæ¡ä»¶:")
    multi_anomaly_threshold = None
    for strength in np.logspace(1, 4, 50):  # ä»10åˆ°10000
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        analyzer = ChernClassAnalyzer(model)
        
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, :4, 0] = strength  # å¤šç‚¹å¼‚å¸¸
        inputs[0, 4:, 0] = -strength  # å¯¹åº”è´Ÿå€¼
        
        outputs = model(inputs)
        phase_transitions = analyzer.detect_topological_phase_transitions()
        
        if len(phase_transitions) > 0:
            multi_anomaly_threshold = strength
            print(f"    ä¸´ç•Œå¼ºåº¦: {strength:.1f}")
            break
    
    # ç±»å‹3: çº¦æŸå¼‚å¸¸
    print("  3. çº¦æŸå¼‚å¸¸ä¸´ç•Œæ¡ä»¶:")
    constraint_threshold = None
    for strength in np.logspace(1, 4, 50):  # ä»10åˆ°10000
        model = CognitiveFiberBundle(vocab_size, d_model=d_model)
        analyzer = ChernClassAnalyzer(model)
        
        inputs = torch.randn(batch_size, vocab_size, d_model)
        inputs[0, 0, :] = torch.ones(d_model) * strength  # ä¸€è‡´æ€§ç ´å
        inputs[0, 1, :] = torch.ones(d_model) * -strength # ä¸€è‡´æ€§ç ´å
        
        outputs = model(inputs)
        phase_transitions = analyzer.detect_topological_phase_transitions()
        
        if len(phase_transitions) > 0:
            constraint_threshold = strength
            print(f"    ä¸´ç•Œå¼ºåº¦: {strength:.1f}")
            break
    
    # å¯è§†åŒ–ç»“æœ
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()
    
    # å¼‚å¸¸å¼ºåº¦ vs æ‹“æ‰‘ç›¸å˜æ•°é‡
    axes[0].semilogx(anomaly_strengths, phase_transitions_count, 'ro-', linewidth=2, markersize=6)
    axes[0].set_title('å¼‚å¸¸å¼ºåº¦ vs æ‹“æ‰‘ç›¸å˜æ•°é‡', fontsize=12)
    axes[0].set_xlabel('å¼‚å¸¸å¼ºåº¦')
    axes[0].set_ylabel('æ‹“æ‰‘ç›¸å˜æ•°é‡')
    axes[0].grid(True, alpha=0.3)
    
    # å¼‚å¸¸å¼ºåº¦ vs é™ˆç±»å˜åŒ–
    axes[1].semilogx(anomaly_strengths, chern_changes, 'bo-', linewidth=2, markersize=6)
    axes[1].set_title('å¼‚å¸¸å¼ºåº¦ vs ç¬¬äºŒé™ˆç±»å˜åŒ–', fontsize=12)
    axes[1].set_xlabel('å¼‚å¸¸å¼ºåº¦')
    axes[1].set_ylabel('ç¬¬äºŒé™ˆç±»å˜åŒ–é‡')
    axes[1].grid(True, alpha=0.3)
    
    # ä¸´ç•Œç‚¹æ ‡æ³¨
    if critical_points:
        critical_strengths = [pt[0] for pt in critical_points]
        critical_changes = [pt[1] for pt in critical_points]
        axes[1].scatter(critical_strengths, critical_changes, color='red', s=100, zorder=5, label='ä¸´ç•Œç‚¹')
        axes[1].legend()
    
    # ä¸åŒå¼‚å¸¸ç±»å‹çš„ä¸´ç•Œé˜ˆå€¼å¯¹æ¯”
    anomaly_types = ['å•ç‚¹å¼‚å¸¸', 'å¤šç‚¹å¼‚å¸¸', 'çº¦æŸå¼‚å¸¸']
    thresholds = [single_anomaly_threshold or 0, multi_anomaly_threshold or 0, constraint_threshold or 0]
    
    bars = axes[2].bar(anomaly_types, thresholds, color=['red', 'orange', 'blue'], alpha=0.7)
    axes[2].set_title('ä¸åŒç±»å‹å¼‚å¸¸çš„ä¸´ç•Œé˜ˆå€¼', fontsize=12)
    axes[2].set_ylabel('ä¸´ç•Œå¼‚å¸¸å¼ºåº¦')
    axes[2].set_yscale('log')
    
    for bar, threshold in zip(bars, thresholds):
        if threshold > 0:
            axes[2].text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.1,
                        f'{threshold:.1f}', ha='center', va='bottom', fontsize=10)
    
    # æ‹“æ‰‘ç›¸å˜æœºåˆ¶ç¤ºæ„å›¾
    axes[3].text(0.5, 0.7, 'æ‹“æ‰‘ç›¸å˜æœºåˆ¶', horizontalalignment='center', 
                verticalalignment='center', transform=axes[3].transAxes, fontsize=14, weight='bold')
    axes[3].text(0.5, 0.5, 'è®¤çŸ¥çº¤ç»´ä¸› â†’ åèº«æ€§è‡ªé€‚åº” â†’ æ‹“æ‰‘éŸ§æ€§ â†’ ä¸´ç•Œç›¸å˜', 
                horizontalalignment='center', verticalalignment='center', 
                transform=axes[3].transAxes, fontsize=12)
    axes[3].text(0.5, 0.3, 'ä¸´ç•Œé˜ˆå€¼: ç³»ç»Ÿä¿æŠ¤æœºåˆ¶å¤±æ•ˆç‚¹', 
                horizontalalignment='center', verticalalignment='center', 
                transform=axes[3].transAxes, fontsize=10)
    axes[3].set_xlim(0, 1)
    axes[3].set_ylim(0, 1)
    
    plt.tight_layout()
    plt.show()
    
    # æ€»ç»“
    print(f"\nğŸ“Š æ‹“æ‰‘ç›¸å˜ä¸´ç•Œæ¡ä»¶æ€»ç»“:")
    print(f"  å•ç‚¹å¼‚å¸¸ä¸´ç•Œé˜ˆå€¼: {single_anomaly_threshold or 'æœªå‘ç°'}")
    print(f"  å¤šç‚¹å¼‚å¸¸ä¸´ç•Œé˜ˆå€¼: {multi_anomaly_threshold or 'æœªå‘ç°'}")
    print(f"  çº¦æŸå¼‚å¸¸ä¸´ç•Œé˜ˆå€¼: {constraint_threshold or 'æœªå‘ç°'}")
    
    if critical_points:
        print(f"\nğŸ¯ ä¸´ç•Œç‚¹åˆ†æ:")
        for i, (strength, chern_change, trans_count) in enumerate(critical_points):
            print(f"  ä¸´ç•Œç‚¹{i+1}: å¼‚å¸¸å¼ºåº¦â‰¥{strength:.1f}æ—¶å‘ç”Ÿæ‹“æ‰‘ç›¸å˜")
    
    print(f"\nğŸ¯ ç†è®ºæ„ä¹‰:")
    print(f"  â€¢ ä¸´ç•Œé˜ˆå€¼ = è®¤çŸ¥ç³»ç»Ÿçš„æ‹“æ‰‘ä¿æŠ¤æé™")
    print(f"  â€¢ è¶…è¿‡é˜ˆå€¼ â†’ æ‹“æ‰‘ç»“æ„å‘ç”Ÿæ ¹æœ¬æ€§æ”¹å˜")
    print(f"  â€¢ ä¸ºAGIå®‰å…¨æ€§æä¾›ç†è®ºè¾¹ç•Œ")
    
    return {
        'critical_points': critical_points,
        'single_anomaly_threshold': single_anomaly_threshold,
        'multi_anomaly_threshold': multi_anomaly_threshold,
        'constraint_threshold': constraint_threshold,
        'anomaly_strengths': anomaly_strengths,
        'phase_transitions': phase_transitions_count,
        'chern_changes': chern_changes
    }

# è¿è¡Œæ‹“æ‰‘ç›¸å˜ä¸´ç•Œæ¡ä»¶æµ‹è¯•
if __name__ == "__main__":
    results = find_topological_phase_transition_thresholds()
