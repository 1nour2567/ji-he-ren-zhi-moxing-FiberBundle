# !pip install transformers==4.35.2 accelerate tiktoken torch  # å¦‚éœ€è¿è¡Œï¼Œè¯·å–æ¶ˆæ³¨é‡Š

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import math
from transformers import AutoTokenizer, AutoModelForCausalLM

# === 1. è®¤çŸ¥çº¤ç»´ä¸›å®‰å…¨å±‚ ===
class CognitiveFiberBundle(nn.Module):
    """è®¤çŸ¥çº¤ç»´ä¸›æ¨¡å‹ - é™ˆç±»æ¯”å€¼å¢å¼ºç‰ˆï¼ˆå…¼å®¹ç‰ˆï¼‰"""
    def __init__(self, d_model=4096):
        super().__init__()
        self.d_model = d_model
        self.connection_form = nn.Parameter(torch.randn(d_model, d_model) * 0.001)
        self.curvature_weight = nn.Parameter(torch.randn(d_model, d_model) * 0.0001)
        self.safety_layer = nn.Linear(d_model, d_model)
        self.norm = nn.LayerNorm(d_model)
    
    def compute_chern_class(self, hidden_states):
        """è®¡ç®—é™ˆç±»"""
        batch_size, seq_len, d = hidden_states.shape
        x_flat = hidden_states.view(-1, d)
        
        connection = self.connection_form.unsqueeze(0).expand(x_flat.shape[0], -1, -1)
        input_diag = torch.diag_embed(torch.tanh(x_flat) * 0.01)
        connection = connection + input_diag
        
        A_squared = torch.bmm(connection, connection)
        A_transposed = connection.transpose(-2, -1)
        A_transposed_squared = torch.bmm(A_transposed, connection)
        curvature = A_squared - A_transposed_squared
        
        trace_F = torch.diagonal(curvature, dim1=-2, dim2=-1).sum(dim=-1)
        first_chern_class = trace_F / (2 * math.pi * 1j)
        F_squared = torch.bmm(curvature, curvature)
        trace_F_squared = torch.diagonal(F_squared, dim1=-2, dim2=-1).sum(dim=-1)
        second_chern_class = (trace_F_squared - trace_F**2) / (8 * math.pi**2)
        chern_ratio = second_chern_class / (first_chern_class.real + 1e-8)
        
        return {
            'first_chern': first_chern_class.real.view(batch_size, seq_len),
            'second_chern': second_chern_class.view(batch_size, seq_len),
            'chern_ratio': chern_ratio.view(batch_size, seq_len),
            'curvature_norm': torch.norm(curvature, dim=[-2, -1]).view(batch_size, seq_len)
        }
    
    def forward(self, hidden_states):
        """å®‰å…¨å¢å¼ºå‰å‘ä¼ æ’­"""
        batch_size, seq_len, d = hidden_states.shape
        topology = self.compute_chern_class(hidden_states)
        
        safe_hidden = self.safety_layer(hidden_states)
        safe_output = self.norm(safe_hidden + hidden_states)
        
        chern_ratio_max = torch.max(torch.abs(topology['chern_ratio']))
        if chern_ratio_max > 1.0:
            risk_mask = torch.abs(topology['chern_ratio']) > 1.0
            safe_output = torch.where(
                risk_mask.unsqueeze(-1), 
                safe_output * 0.9,
                safe_output
            )
        
        return safe_output, topology

# === 2. å®‰å…¨å¢å¼ºçš„Qwenæ¨¡å‹ ===
class SafeQwenModel(nn.Module):
    """å®‰å…¨å¢å¼ºç‰ˆQwenæ¨¡å‹"""
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.safety_threshold = 0.8
        
        self.cognitive_bundles = nn.ModuleList([
            CognitiveFiberBundle(d_model=base_model.config.hidden_size)
            for _ in range(base_model.config.num_hidden_layers)
        ])
    
    def forward(self, input_ids, attention_mask=None):
        """å®‰å…¨å¢å¼ºå‰å‘ä¼ æ’­"""
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        
        last_hidden = outputs.hidden_states[-1]
        safe_hidden, topology = self.cognitive_bundles[-1](last_hidden)
        
        outputs.hidden_states = outputs.hidden_states[:-1] + (safe_hidden,)
        
        safety_status = {
            'chern_ratio_mean': torch.mean(torch.abs(topology['chern_ratio'])).item(),
            'chern_ratio_max': torch.max(torch.abs(topology['chern_ratio'])).item(),
            'is_safe': torch.max(torch.abs(topology['chern_ratio'])).item() < self.safety_threshold
        }
        
        return outputs, safety_status

# === 3. ä¿®å¤åçš„å®‰å…¨äº¤äº’æ¥å£ï¼ˆè§£å†³pad_tokené—®é¢˜ï¼‰===
class SafeQwenInterface:
    """å®‰å…¨Qwenäº¤äº’æ¥å£"""
    def __init__(self, model_path="Qwen/Qwen-7B"):
        print("ğŸš€ æ­£åœ¨åŠ è½½å®‰å…¨å¢å¼ºç‰ˆQwen...")
        
        # ä¿®å¤ï¼šå…ˆåŠ è½½tokenizerï¼Œç„¶åæ­£ç¡®è®¾ç½®pad_token
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿pad_tokenå­˜åœ¨
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print("ğŸ”§ å·²è®¾ç½®pad_tokenä¸ºeos_token")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # åˆ›å»ºå®‰å…¨å¢å¼ºæ¨¡å‹
        self.safe_model = SafeQwenModel(self.base_model)
        print("âœ… å®‰å…¨å¢å¼ºQwenåŠ è½½å®Œæˆï¼")
    
    def generate_safe_response(self, prompt, max_length=200):
        """ç”Ÿæˆå®‰å…¨å“åº”ï¼ˆä¿®å¤paddingé—®é¢˜ï¼‰"""
        # ä¿®å¤ï¼šä¸ä½¿ç”¨paddingï¼Œç›´æ¥ç¼–ç 
        inputs = self.tokenizer(prompt, return_tensors="pt")
        input_ids = inputs['input_ids'].to(self.base_model.device)
        attention_mask = inputs.get('attention_mask', torch.ones_like(input_ids)).to(self.base_model.device)
        
        with torch.no_grad():
            outputs, safety_status = self.safe_model(input_ids, attention_mask)
            safe_logits = self.base_model.lm_head(outputs.hidden_states[-1])
            next_token_logits = safe_logits[:, -1, :]
            
            # å®‰å…¨è¿‡æ»¤
            if not safety_status['is_safe']:
                next_token_logits = torch.where(
                    torch.abs(next_token_logits) > 10.0,
                    next_token_logits * 0.5,
                    next_token_logits
                )
            
            # é‡‡æ ·ï¼ˆä½¿ç”¨temperatureæ§åˆ¶éšæœºæ€§ï¼‰
            probs = torch.softmax(next_token_logits / 0.8, dim=-1)  # temperature=0.8
            next_token = torch.multinomial(probs, num_samples=1)
        
        response = self.tokenizer.decode(next_token[0], skip_special_tokens=True)
        
        return {
            'response': response,
            'safety_status': safety_status,
            'raw_logits': safe_logits
        }
    
    def get_safety_report(self, prompt, response):
        """è·å–å®‰å…¨æŠ¥å‘Š"""
        inputs = self.tokenizer(prompt + response, return_tensors="pt")
        input_ids = inputs['input_ids'].to(self.base_model.device)
        attention_mask = inputs.get('attention_mask', torch.ones_like(input_ids)).to(self.base_model.device)
        
        with torch.no_grad():
            outputs, safety_status = self.safe_model(input_ids, attention_mask)
        
        return f"""
ğŸ›¡ï¸ å®‰å…¨å¢å¼ºç‰ˆQwenåˆ†ææŠ¥å‘Š
========================
ğŸ“‹ è¾“å…¥: {prompt}
ğŸ’¬ è¾“å‡º: {response}
ğŸ“Š å®‰å…¨æŒ‡æ ‡:
   â€¢ é™ˆç±»æ¯”å€¼å‡å€¼: {safety_status['chern_ratio_mean']:.4f}
   â€¢ é™ˆç±»æ¯”å€¼æœ€å¤§å€¼: {safety_status['chern_ratio_max']:.4f}
   â€¢ å®‰å…¨çŠ¶æ€: {'âœ… å®‰å…¨' if safety_status['is_safe'] else 'âš ï¸ é£é™©'}
ğŸ¯ ä¿æŠ¤æœºåˆ¶:
   â€¢ è®¤çŸ¥çº¤ç»´ä¸›å®æ—¶ç›‘æ§
   â€¢ é™ˆç±»æ¯”å€¼è‡ªåŠ¨ä¿®å¤
   â€¢ å±é™©è¾“å‡ºæ¦‚ç‡æŠ‘åˆ¶
        """

# === 4. ä¿®å¤åçš„æµ‹è¯•ç¤ºä¾‹ ===
def demo_safe_qwen():
    """æ¼”ç¤ºå®‰å…¨å¢å¼ºQwenåŠŸèƒ½"""
    safe_qwen = SafeQwenInterface()
    
    test_prompts = [
        "å¦‚ä½•è®©ä¸–ç•Œæ›´ç¾å¥½ï¼Ÿ",
        "å¦‚ä½•è®©äººç±»ç”Ÿæ´»å¾—æ›´å¥½ï¼Ÿ",
        "å¦‚ä½•è§£å†³æ°”å€™å˜åŒ–ï¼Ÿ"
    ]
    
    print("\nğŸ” å®‰å…¨å¢å¼ºç‰ˆQwenæµ‹è¯•å¼€å§‹...")
    for prompt in test_prompts:
        print(f"\nğŸ“ é—®é¢˜: {prompt}")
        
        result = safe_qwen.generate_safe_response(prompt)
        response = result['response']
        safety_status = result['safety_status']
        
        print(f"ğŸ’¬ å›ç­”: {response}")
        print(f"ğŸ“Š å®‰å…¨çŠ¶æ€: é™ˆç±»æ¯”å€¼={safety_status['chern_ratio_mean']:.4f} ({'âœ… å®‰å…¨' if safety_status['is_safe'] else 'âš ï¸ é£é™©'})")
        
        report = safe_qwen.get_safety_report(prompt, response)
        print(report)
        print("-" * 50)

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_safe_qwen()
